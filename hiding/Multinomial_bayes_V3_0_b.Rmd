---
title: "Multinomial Naive Bayes Workflow Ver 3.0.b."
author: "Dan Johnson"
date: "2/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Summary
This is an alternative version of my basic NB workflow for classifying TA comments. The code is slightly different than what was used for Categorical NB testing, but takes in text using the same command set as NB-Ver 2.0. It is based on a tutorial located here:
[https://rpubs.com/cen0te/naivebayes-sentimentpolarity]

This alternative code allows for true multinomial analysis, by switching between using binary yes/no categorization of words' presence or absence, and the actual frequencies. There is a trade-off though; to use non-categorical values, I can only have 2 assignment bins: in-group versus out-group. If I use multiple groups, the data fail to parse correctly. If I want to look at multiple classification bins at the same time, I have to use the categorical values method.

The point seems moot overall, since categorical NB seems to perform better than true multinomial NB. For both Subject and Structure, I reach 75-84% overall accuracy if I sort comments into ingroup/outgroup categories using binary (yes/no) word frequencies. Accuracy drops to ~65% if I use the raw numeric frequencies. Changing the Laplace value does not affect overall performance.

Naive Bayes permutations seem to plateau at 80-85%, regardless of permutations. 

####\  

##Initial Setup
Calling the required libraries.

```{r}
library(tm)
library(RTextTools)
library(e1071)
library(dplyr)
library(caret)
library(doMC)
registerDoMC(core=detectCores())

library(quanteda)
library(RColorBrewer)
library(ggplot2)

#library(tidyverse) 
#library(tidytext)
#library(tidyr)
```

####\  

##Version 3.0.b of Naive Bayes (NB) Protocol
###Initial Data Input
```{r}
#Read in TA comments from CSV file. 
#For convenience, I am reading in a shorter CSV file created previously using ver.2.0 code. This CSV file only lists 
#whether a comment is in or out of the 2 main groups, "Subject:3_technical" and "Structure:4_specific"

subset_M <- read.csv("/Users/danjohnson/Dropbox/Coding_Tools/R_Environment/R_Projects/ta_comments_FLC/comments_binary.csv", stringsAsFactors=FALSE)
glimpse(subset_M)
```


```{r}
set.seed(2012) #Randomizer
subset_M <- subset_M[sample(nrow(subset_M)), ]
glimpse(subset_M)
```

```{r}
text.corpus<-corpus(subset_M$text)
docvars(text.corpus)<-subset_M$subject
glimpse(text.corpus)
```

```{r}
ingroup.plot<-corpus_subset(text.corpus,docvar1=="ingroup")
ingroup.plot<-dfm(ingroup.plot, tolower=TRUE,remove_punct=TRUE,remove_twitter=TRUE,remove_numbers=TRUE,remove=stopwords(source="smart"))
ingroup.col <-brewer.pal(10, "BrBG")
textplot_wordcloud(ingroup.plot, min_count=16, color=ingroup.col)
title("Ingroup Wordcloud", col.main="grey14")

outgroup.plot<-corpus_subset(text.corpus,docvar1=="outgroup")
outgroup.plot<-dfm(outgroup.plot, tolower=TRUE,remove_punct=TRUE,remove_twitter=TRUE,remove_numbers=TRUE,remove=stopwords(source="smart"))
outgroup.col <-brewer.pal(10, "BrBG")
textplot_wordcloud(outgroup.plot, min_count=16, color=outgroup.col)
title("Outgroup Wordcloud", col.main="grey14")
```

Let’s separate the training and test data-
```{r}
#separating Train and test data
text.train<-subset_M[1:7005,]
text.test<-subset_M[7006:nrow(subset_M),]

msg.dfm <- dfm(text.corpus, tolower = TRUE)  #generating document freq matrix
msg.dfm <- dfm_trim(msg.dfm, min_termfreqError = 5, min_docfreq = 3)  
msg.dfm <- dfm_weight(msg.dfm) 

head(msg.dfm)
```


#trining and testing data of dfm 
```{r}
msg.dfm.train<-msg.dfm[1:7005,]

msg.dfm.test<-msg.dfm[7006:nrow(subset_M),]

```




Training the Naive Bayes classifier-

nb.classifier<-textmodel_NB(msg.dfm.train,spam.train[,1])
nb.classifier
## Fitted Naive Bayes model:
## Call:
##  textmodel_NB.dfm(x = msg.dfm.train, y = spam.train[, 1])
## 
## 
## Training classes and priors:
## spam  ham 
##  0.5  0.5 
## 
##        Likelihoods:      Class Posteriors:
## 30 x 4 Matrix of class "dgeMatrix"
##                 spam          ham       spam       ham
## you     5.001507e-03 0.0096798156 0.34067144 0.6593286
## have    4.322289e-03 0.0042303673 0.50537386 0.4946261
## 1       2.695748e-03 0.0009529526 0.73882413 0.2611759
## new     3.492485e-03 0.0010753934 0.76457487 0.2354251
## .       6.965338e-03 0.0168302131 0.29271598 0.7072840
## please  2.339097e-03 0.0011593603 0.66860811 0.3313919
## call    1.058603e-02 0.0021859571 0.82884759 0.1711524
## i       8.439760e-04 0.0112106647 0.07001254 0.9299875
## wait    1.860817e-04 0.0011538316 0.13887596 0.8611240
## for     5.699340e-03 0.0045025239 0.55865674 0.4413433
## hope    2.334040e-04 0.0017258550 0.11912872 0.8808713
## tonight 1.137075e-04 0.0011106417 0.09287182 0.9071282
## too     3.802754e-05 0.0017024748 0.02184860 0.9781514
## bad     1.232420e-04 0.0006045270 0.16934219 0.8306578
## as      1.339518e-03 0.0020699791 0.39287852 0.6071215
## well    2.938089e-04 0.0017334850 0.14492664 0.8550734
## but     2.528948e-04 0.0043933716 0.05442968 0.9455703
## rock    3.802754e-05 0.0002684845 0.12406542 0.8759346
## night   3.003905e-04 0.0017976398 0.14317739 0.8568226
## anyway  3.802754e-05 0.0005405216 0.06572915 0.9342709
## going   1.538819e-04 0.0023951976 0.06036762 0.9396324
## a       7.856726e-03 0.0064918622 0.54756091 0.4524391
## now     6.254232e-03 0.0028758075 0.68501697 0.3149830
## good    6.723203e-04 0.0030342352 0.18138681 0.8186132
## speak   8.003838e-04 0.0004728416 0.62862694 0.3713731
## to      1.113210e-02 0.0075761991 0.59503541 0.4049646
## soon    2.642059e-04 0.0010608467 0.19939274 0.8006073
## today   1.120666e-03 0.0019041688 0.37048833 0.6295117
## is      4.451802e-03 0.0058153446 0.43359683 0.5664032
## accept  3.802754e-05 0.0003188419 0.10655871 0.8934413


The model outputs the Probabilities of the message being Spam or ham.
Let’s Test the Model

pred<-predict(nb.classifier,msg.dfm.test)

#generating a confusion matrix

# use pred$nb.predicted to extract the class labels
table(predicted=pred$nb.predicted,actual=spam.test[,1])
##       actual
## predicted ham spam
##      ham  952    7
##      spam  16  140

16 text examples wrongly classified for ham and 7 examples wrongly classified for spam.
In the Confusion matrix,the diagonals are the correctly classified examples and the off-diagonals are the incorrectly classified text instances.

Now let’s calculate the accuracy of the model –

#acccuracy of the classifier on Test data
mean(pred$nb.predicted==spam.test[,1])*100
## [1] 97.93722

Now 98% accuracy is a good amount of accuracy on unseen random test data.










```{r}
subset_M$subject <- as.factor(subset_M$subject)
subset_M$structure <- as.factor(subset_M$structure)
```

```{r}
corpus <- Corpus(VectorSource(subset_M$text))
corpus
inspect(corpus[4:6])
```

```{r}
#In my code this block is split into substeps to make A/B testing easier.
corpus.clean <- corpus %>%
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords(kind="en")) %>%
  tm_map(stripWhitespace)
```
```{r}
corpus.dfm <- dfm(corpus.clean, tolower=TRUE)

# Inspect the dtm
#inspect(dtm[40:50, 10:15])
```

```{r}
dtm <- DocumentTermMatrix(corpus.clean)

# Inspect the dtm
inspect(dtm[40:50, 10:15])
```

```{r}
subset_M.train <- subset_M[1:7005,]
subset_M.test <- subset_M[7006:9340,]

dtm.train <- dtm[1:7005,]
dtm.test <- dtm[7006:9340,]

corpus.clean.train <- corpus.clean[1:7005]
corpus.clean.test <- corpus.clean[7006:9340]
```

```{r}
dim(dtm.train)
```

```{r}
fivefreq<- findFreqTerms(dtm.train, 5)
length((fivefreq))

dtm.train.nb <-DocumentTermMatrix(corpus.clean.train, control=list(dictionary=fivefreq))

dim(dtm.train.nb)

dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))

dim(dtm.train.nb)
```

Here are the two blocks of code that switch between using a variation of the multinomial Naive Bayes algorithm known as binarized (boolean feature) Naive Bayes (from Dan Jurafsky) versus non-binarized. In this method, the term frequencies are replaced by Boolean presence/absence features. The logic behind this is that for sentiment classification, word occurrence matters more than word frequency. For my purposes, either might work, so both must be tested; put "r" in the opening brackets for whichever is needed.

```{r}
# Function to convert the word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}
```

```{}
# Function to KEEP word frequencies while still using the same variable names and code structure.
convert_count <- function(x) {
  y <- x
}
```

```{r}
# Apply the convert_count function to get final training and testing DTMs (whichever was chosen earlier)
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
```

```{r}
# Train the classifier. Not sure what system.time is needed for, except to show when block finishes.
system.time( classifier <- naiveBayes(trainNB, subset_M.train$subject, laplace = 1) )
system.time( classifier2 <- naiveBayes(trainNB, subset_M.train$structure, laplace = 1) )
```


##Testing the Predictions

```{r}
# Use the NB classifier we built to make predictions on the test set. Again, why use system.time?
system.time( pred <- predict(classifier, newdata=testNB) )
system.time( pred2 <- predict(classifier2, newdata=testNB) )
```

```{r}
# Create a truth table by tabulating the predicted class labels with the actual class labels 
line<-c("******************")
print("Subject")
table("Predictions"= pred,  "Actual" = subset_M.test$subject )
print(line)
print("Structure")
table("Predictions2"= pred2,  "Actual" = subset_M.test$structure )
```

```{r}
# Prepare the confusion matrix
conf.mat <- confusionMatrix(pred, subset_M.test$subject)
conf.mat
glimpse(line)

conf.mat$byClass
glimpse(line)

conf.mat$overall
glimpse(line)

conf.mat$overall['Accuracy']

glimpse(line)
glimpse(line)

# Prepare the confusion matrix
conf.mat2 <- confusionMatrix(pred2, subset_M.test$structure)
conf.mat2
glimpse(line)

conf.mat2$byClass
glimpse(line)

conf.mat2$overall
glimpse(line)

conf.mat2$overall['Accuracy']

```




###OPTIONAL Output
To make a table showing predicted categories for comments based on the Naive Bayes test, versus the actual label entered by a human observer, merge the two datasets into a single data frame. The full data frame will be too long to display; use "head" to check format, then output the full dataset to CSV for further review. By default the CSV file should be saved to the current working directory.

table("Predictions"= pred,  "Actual" = subset_M.test$subject )
print(line)
print("Structure")
table("Predictions2"= pred2,  "Actual" = subset_M.test$structure )

```{r}
predicted_category_subject <- data.frame(pred,subset_M.test$subject)
head(predicted_category_subject)
write.csv(predicted_category_subject, file = "PredictedCategorySubject_Data.csv")
```

```{r}
predicted_category_structure <- data.frame(pred,subset_M.test$structure)
head(predicted_category_structure)
write.csv(predicted_category_structure, file = "PredictedCategoryStructure_Data.csv")
```


***
START COMMENTS, OLD work notes
***







***
END COMMENTS
***

