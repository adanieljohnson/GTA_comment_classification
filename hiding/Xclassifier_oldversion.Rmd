---
title: "Classifier Development"
author: "Dan Johnson"
date: "12/17/2018"
bibliography: library.bib
biblio-style: apalike
---
```{r, child="_setup.Rmd"}
```
#Overview
This page has three sections. **Part 1** describes the target dataset for this project. **Part 2** outlines the data pre-checks and import process. **Part 3** describes post-import validation developed for the target dataset, and how selected elements within the dataset are recoded for analysis.  

Initial visual exploration of the dataset is described here: https://adanieljohnson.github.io/default_website/initial_exploration.html

These linked pages summarize work to date using categorical Naive Bayes as the text classification method. They provided a baseline from which I am evaluating other approaches.

*  Baseline Method: https://adanieljohnson.github.io/default_website/Naive_bayesV2b.html
*  Randomization Method: https://adanieljohnson.github.io/default_website/Naive_bayesV2r.html
*  In/Outgroup Binning: https://adanieljohnson.github.io/default_website/Naive_bayesV2yn.html
*  In/Outgroup Binning: https://adanieljohnson.github.io/default_website/Naive_bayes_Smry.html


Other pages provide the theoretical background for the selected methods. Each deeper analysis method is described in detail on its own separate page. These are listed on the **[LINK] Project Outline** page.

####\  


##Initial Library and Data Import
```{r Libraries_1}
# Call in tidyverse, other required libraries.
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
```

```{r}
#This block reads in full CSV into a starting dataframe named "base_data".
#Subsequent analyses are performed on extracted subsets of "base_data".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anonVERB.csv')
```

If dataset imports correctly, there will be a dataframe of ~11,000 rows and 28 columns named as listed in the **Data Pre-Checks** table above.

####\  

#Part 3. Post-Import Checks
Given some data points may be entered incorrectly by instructors or students, these post-import data checks help ensure data have been properly coded and columns contain valid entries only. (This process can be automated using the ```janitor``` data cleaning package.)

```{r}
#"Course" column should only have entries that correspond to 113, 114, 214, NA. 
course<-unique(base_data$course)

#Entries in "TAs" column should match the anonymous IDs that correspond to the TAs assigned to the relevant courses in the semester being analyzed. 
#If there are incorrect or extra anonymous IDs in this list, the mostly likely source of the error is improper de-identification of the Excel file prior to import.
ta<-unique(base_data$ta)

#"Lab" should only have entries that match the allowed limited keywords in codebook.
lab<-unique(base_data$lab)

#It is impractical to validate all randomized identifiers. Instead look at the NUMBER of unique anonymous student IDs. Quantity should be within 10% of (but not more than) the combined enrollment of the 3 target courses for the semester.
student<-unique(base_data$student)

#Check that the list of "code.subject" topics here matches allowed terms in codebook.
subject<-unique(base_data$code.subject)

#Check that the list of "code.structure" terms extracted from the dataset and displayed here matches allowed terms in codebook.
structure<-unique(base_data$code.structure)
```
####\  

##Post-Import Processing
Comment labels used in the project codebook contain punctuation and spacing that complicates subsequent analysis. Rather than modify the project codebook or external processes, it is simpler to recode the problematic labels after import. For example, the following code extracts a subset of rows from the full dataset, reduces those to 3 columns, and renames the **comment.subject** column as **subject**. Four problematic code labels in the subject column are replaced with simpler terms. 

```{r}
#Read in TA comments from CSV file.
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')

#Select rows representing the sub-groups to compare. 
comments_subset <- filter(base_data,code.subject=="1. Basic Criteria"|code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")

#Reduce larger dataframe to 3 required columns of data (subject, structure, and text of comment), and put columns in order needed.
comments_raw <- comments_subset %>% select(23,24,22)

#Rename the columns.
names(comments_raw)[1] <- "subject"
names(comments_raw)[2] <- "structure"
names(comments_raw)[3] <- "text"

#Simplify coding terms used in "subject" column.
comments_raw[,1] <- ifelse(comments_raw[,1] == "1. Basic Criteria","1_basic", ifelse(comments_raw[,1] == "2. Writing Quality","2_writing", ifelse(comments_raw[,1] == "3. Technical and Scientific","3_technical", ifelse(comments_raw[,1] == "4. Logic and Thinking","4_logic",99))))

#Change "subject" element from character to a factor for analysis.
comments_raw$subject <- factor(comments_raw$subject)
str(comments_raw$subject)
table(comments_raw$subject)
```

Most in-depth analyses will have a similar extraction and post-processing code block. In practice, it has proven easier to modify the post-processing code than to maintain multiple separate copies of the original CSV dataset for the different analyses.