---
title: "Multinomial Naive Bayes Workflow Ver 1.0."
author: "Dan Johnson"
date: "2/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Summary
This is **Version 1.0** of a basic workflow for classifying TA comments using a multinomial naive Bayes model. It pre-processes text using the same command set as NB-Ver 2.0. This version replaces categorical naive Bayes with a multinomial classifier.

####\  

##Initial Setup
Calling the required libraries.

```{r}

library(tm)
library(RTextTools)
library(e1071)
library(dplyr)
library(caret)
library(doMC)
registerDoMC(core=detectCores())

#library(tidyverse)
#library(tidytext)
#library(tidyr)
#library(SnowballC)
#library(wordcloud)
#library(gmodels)
```

####\  

##Version 2.0.r of Naive Bayes (NB) Protocol
###Initial Data Input
```{r}
#Read in TA comments from CSV file.
subset_M <- read.csv("/Users/danjohnson/Dropbox/Coding_Tools/R_Environment/R_Projects/ta_comments_FLC/comments_subset.csv", stringsAsFactors=FALSE)
glimpse(subset_M)
```
```{r}
set.seed(1)
subset_M <- subset_M[sample(nrow(subset_M)), ]
subset_M <- subset_M[sample(nrow(subset_M)), ]
glimpse(subset_M)
```

```{r}
subset_M$subject <- as.factor(subset_M$subject)
subset_M$structure <- as.factor(subset_M$structure)
```

```{r}
corpus <- Corpus(VectorSource(subset_M$text))
corpus
inspect(corpus[1:3])
```

```{r}
corpus.clean <- corpus %>%
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords()) %>%
  tm_map(stripWhitespace)
```

```{r}
dtm <- DocumentTermMatrix(corpus.clean)
# Inspect the dtm
inspect(dtm[40:50, 10:15])
```


```{r}
subset_M.train <- subset_M[1:7005,]
subset_M.test <- subset_M[7006:9340,]

dtm.train <- dtm[1:7005,]
dtm.test <- dtm[7006:9340,]

corpus.clean.train <- corpus.clean[1:7005]
corpus.clean.test <- corpus.clean[7006:9340]
```

```{r}
dim(dtm.train)
```

```{r}
fivefreq<- findFreqTerms(dtm.train, 5)
length((fivefreq))

dtm.train.nb <-DocumentTermMatrix(corpus.clean.train, control=list(dictionary=fivefreq))

dim(dtm.train.nb)

dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))

dim(dtm.train.nb)
```

We use a variation of the multinomial Naive Bayes algorithm known as binarized (boolean feature) Naive Bayes due to Dan Jurafsky. In this method, the term frequencies are replaced by Boolean presence/absence features. The logic behind this being that for sentiment classification, word occurrence matters more than word frequency.
```{r}
# Function to convert the word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}
```

```{r}
# Function to avoid converting word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- x
}
```

# Apply the convert_count function to get final training and testing DTMs
```{r}
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
```

```{r}
# Train the classifier
system.time( classifier <- naiveBayes(trainNB, subset_M.train$subject, laplace = 1) )
#system.time( classifier <- naiveBayes(trainNB, subset_M.train$structure, laplace = 1) )
```


Testing the Predictions
```{r}
# Use the NB classifier we built to make predictions on the test set.
system.time( pred <- predict(classifier, newdata=testNB) )
```

```{r}
# Create a truth table by tabulating the predicted class labels with the actual class labels 
table("Predictions"= pred,  "Actual" = subset_M.test$subject )
```

```{r}
# Prepare the confusion matrix
conf.mat <- confusionMatrix(pred, subset_M.test$subject)
conf.mat
print ("   ")

conf.mat$byClass
print ("   ")

conf.mat$overall
print ("   ")

conf.mat$overall['Accuracy']

```





https://rpubs.com/cen0te/naivebayes-sentimentpolarity




###Extract, Organize Data Subset
Extract comments and classification data subset
```{r}
#Select rows representing the sub-groups to compare. 
comments_subsetVERB <- filter(base_dataVERB,code.subject=="1_basic"|code.subject=="2_writing"|code.subject=="3_technical"|code.subject=="4_logic")

#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
comments_rawVERB <- comments_subsetVERB %>% select(23,24,22)

#Rename the columns.
names(comments_rawVERB)[1] <- "subject"
names(comments_rawVERB)[2] <- "structure"
names(comments_rawVERB)[3] <- "text"

#Change "subject" element from character to a factor for analysis.
comments_rawVERB$subject <- factor(comments_rawVERB$subject)
str(comments_rawVERB$subject)
table(comments_rawVERB$subject)
```

```{r}
#Change "structure" element from character to a factor for analysis.
comments_rawVERB$structure <- factor(comments_rawVERB$structure)
str(comments_rawVERB$structure)
table(comments_rawVERB$structure)
```

```{r}
#Generate a vector of ~10,000 numbers (be sure it can be generated consistently)
#Adjust the number so it matches the number of rows in the original data frame
#Next, set the seed for random numbers, and randomize the order of the vector
vector <- 1:9340
set.seed(123) #change this seed value, and the data are randomized in a new sequence.
vector <- sample(vector)
glimpse(vector)
```

```{r}
#Append the vector as a new column to "comments_rawVERB" with name "randomizer"
#Sort rows of dataframe according to "randomizer" column using dplyer 'arrange' function. 
comments_rawVERB$randomizer <- c(vector)
comments_rawVERB_R <-comments_rawVERB %>% arrange(randomizer)
print(comments_rawVERB_R)

```

Convert data set to a volative corpus using "tm" library.
```{r}
#Create the volatile corpus that contains the "text" vector from data frame.
comments_corpusVERB_R <- VCorpus(VectorSource(comments_rawVERB_R$text))
print(comments_corpusVERB_R)

#Check out the first few text comments in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpusVERB_R[1:3])

#Use "as.character" function to see what a single text comment looks like.
as.character(comments_corpusVERB_R[[5]])
```

###Text Data Transformations
The text data transforms are separated out as individual "tm_map"" and "content_transformer" functions to simplify the process of testing various combinations. Conversion to lower case and removing punctuation are the baseline set. Switch between "{r}" and "{}" to turn the other transformation blocks off or on. Removing extra white spaces must be the last transformation, and is required.

```{r}
#Convert text to all lower case letters.
comments_corpus_cleanVERB_R <- tm_map(comments_corpusVERB_R, content_transformer(tolower))

#Remove punctuation using the "removePunctuation" function. 
#This step removes evidence of questions, so may remove data.
comments_corpus_cleanVERB_R <- tm_map(comments_corpus_cleanVERB_R, removePunctuation)

#Remove numerals. 
#I am not sure this is beneficial; does it remove evidence indicating technical comments?
comments_corpus_cleanVERB_R <- tm_map(comments_corpus_cleanVERB_R, removeNumbers)

#Stopword removal. 
#This is a standard cleanup step, but again may remove useful terms.
#The "stopwords" file is a convention; other files can be substituted for it
comments_corpus_cleanVERB_R <- tm_map(comments_corpus_cleanVERB_R, removeWords, stopwords())

#The option to use stemming has been removed from this version.

#Final step removes white space from the document. This is NOT an optional step.
comments_corpus_cleanVERB_R <- tm_map(comments_corpus_cleanVERB_R, stripWhitespace)

#Look at an example of cleaned text comment to see if it fits what is expected.
as.character((comments_corpus_cleanVERB_R[[5]]))
```

####\  

##Basic Naive Bayes (NB) Method Using N-grams
This code block uses tm's NLP commands. The same block can be used to assess 1-, 2-, or 3-grams. Only the "#" variable in "(words(x), #)" below needs to be changed. 

```{r}
NLP_Tokenizer <- function(x) {
      unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}

comments_dtm_1gram <- DocumentTermMatrix(comments_corpus_cleanVERB_R, control=list(tokenize = NLP_Tokenizer))
comments_dtm_1gram_tidy <- tidy(comments_dtm_1gram)
inspect(comments_dtm_1gram)
```

####\  

###Data Preparation
Split data into 75% training and 25% testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data. 

```{}
.75 * 9340 #Number of rows in the dataset; product is #rows for training
.25 * 9340 #Product is #rows for testing set
```

This code uses the comments in the order they are listed in the original file. Block for randomizing them was done earlier.
```{r}
comments_dtm_train <- comments_dtm_1gram[1:7005, ]
comments_dtm_test <- comments_dtm_1gram[7006:9340, ]
```

Save vectors labeling rows in the training and testing vectors
```{r}
comments_train_labels_subject <- comments_rawVERB_R[1:7005, ]$subject
comments_test_labels_subject <- comments_rawVERB_R[7006:9340,]$subject
```

```{r}
comments_train_labels_structure <- comments_rawVERB_R[1:7005, ]$structure
comments_test_labels_structure <- comments_rawVERB_R[7006:9340,]$structure
```

Make sure that the proportion of each sub-category is similar in the training and testing data sets.
```{r}
prop.table(table(comments_train_labels_subject))
prop.table(table(comments_test_labels_subject))
```

```{r}
prop.table(table(comments_train_labels_structure))
prop.table(table(comments_test_labels_structure))
```

####\  

###Preparation for Naive Bayes
Remove words from the matrix that appear less than 5 times.
```{r}
comments_freq_words <- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)
```

Limit Document Term Matrix to only include words in the comments_freq_words vector. Using all the rows, but we want to limit the columns to these words in the frequency vector.
```{r}
comments_dtm_freq_train <- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test <- comments_dtm_test[ , comments_freq_words]
```

The basic e1071 naive bayes classifier works with categorical features, so the matrix must be converted "yes" and "no" categorical variables. This is done using a **convert_counts function** and applying it to the data. This replaces values greater than 0 with yes, and values not greater than 0 with no. 
```{r}
convert_counts2 <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

comments_train <- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test <- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)
```

The resulting matrices have cells indicating "yes" or "no" if the word represented by the column appears in the text comment represented by the row.

####\  

###Train Model, Predict, Evaluate for Subject
This block uses the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group 1_basic, 2_writing, 3_technical, or 4_logic. It evaluates the prediction with the actual data using a crosstable from the gmodels package.
```{r}
comments_classifier_subject <- naiveBayes(comments_train, comments_train_labels_subject)
comments_test_pred_subject <- predict(comments_classifier_subject, comments_test)
CrossTable(comments_test_pred_subject, comments_test_labels_subject, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

The cross table lists how many of the predicted categories are in their correct categories. Adding diagonally and dividing by total count gives the percent accuracy at predicting subject.
```{r}
print("Overall subject accuracy")
(43+473+1131+184)/23.35  #Need to find a way to calculate this automatically. 
```


###Train Model, Predict, Evaluate for structure
This is the same block as above but uses the "structure" data column.

```{r}
comments_classifier_structure <- naiveBayes(comments_train, comments_train_labels_structure)
comments_test_pred_structure <- predict(comments_classifier_structure, comments_test)
CrossTable(comments_test_pred_structure, comments_test_labels_structure, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

Again, a cross table shows how many of the predicted categories match the correct categories for structure.

```{r}
print("Overall structure accuracy")
(22+488+292+518+33+10)/23.35  #Need to find a way to calculate this automatically. 
```

###OPTIONAL Output
To make a table showing predicted categories for comments based on the Naive Bayes test, versus the actual label entered by a human observer, merge the two datasets into a single data frame. The full data frame will be too long to display; use "head" to check format, then output the full dataset to CSV for further review. By default the CSV file should be saved to the current working directory.

```{r}
predicted_category_subject <- data.frame(comments_test_pred_subject,comments_test_labels_subject)
head(predicted_category_subject)
write.csv(predicted_category_subject, file = "PredictedCategorySubject_Data.csv")
```

```{r}
predicted_category_structure <- data.frame(comments_test_pred_structure,comments_test_labels_structure)
head(predicted_category_structure)
write.csv(predicted_category_structure, file = "PredictedCategoryStructure_Data.csv")
```


***
START COMMENTS, OLD work notes
***


```{r}
#Creates a data frame that compares predicted and actual labels for subject and structure
tally <- data.frame("subject_label"=c(comments_test_labels_subject), "subject_predicted"=c(comments_test_pred_subject), "structure_label"=c(comments_test_labels_structure), "structure_predicted"=c(comments_test_pred_structure))
#tally <- as.numeric(as.character(tally))
```


```{r}
if_else (tally(comments_test_labels_subject) != tally(comments_test_pred_subject),"YES","no")

#glimpse(tally)

#tally %>% mutate(match_subject=comments_test_labels_subject - comments_test_pred_subject)
#tally %>% mutate(match_structure=comments_test_labels_structure - comments_test_pred_structure)
```

***
END COMMENTS
***

