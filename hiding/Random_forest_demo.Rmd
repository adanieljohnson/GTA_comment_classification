---
title: "Demo of Random Forest Method Ver 1.0.a."
author: "Dan Johnson"
date: "2/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Summary
This is testing the random forest method described here:
[https://datascienceplus.com/random-forests-in-r/]

####\  

##Initial Setup
Calling the required libraries.

```{r}
#library(randomForest) #tools
#library(RTextTools) #demo data. Package which contains the Boston housing dataset
#library(MASS)
attach(Boston)
set.seed(101)

dim(Boston)
```

```{r}
#training Sample with 300 observations
train=sample(1:nrow(Boston),300)
#?Boston  #to search on the dataset

Boston.rf=randomForest(medv ~ . , data = Boston , subset = train)
Boston.rf
```

```{r}
plot(Boston.rf)
```

```{r}
oob.err=double(13)
test.err=double(13)

#mtry is no of Variables randomly chosen at each split
for(mtry in 1:13) 
{
  rf=randomForest(medv ~ . , data = Boston , subset = train,mtry=mtry,ntree=400) 
  oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
  
  pred<-predict(rf,Boston[-train,]) #Predictions on Test Set for each Tree
  test.err[mtry]= with(Boston[-train,], mean( (medv - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
  
}
```

```{r}
test.err
oob.err
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
```










```{r}
ingroup.plot<-corpus_subset(text.corpus,docvar1=="ingroup")
ingroup.plot<-dfm(ingroup.plot, tolower=TRUE,remove_punct=TRUE,remove_twitter=TRUE,remove_numbers=TRUE,remove=stopwords(source="smart"))
ingroup.col <-brewer.pal(10, "BrBG")
textplot_wordcloud(ingroup.plot, min_count=16, color=ingroup.col)
title("Ingroup Wordcloud", col.main="grey14")

outgroup.plot<-corpus_subset(text.corpus,docvar1=="outgroup")
outgroup.plot<-dfm(outgroup.plot, tolower=TRUE,remove_punct=TRUE,remove_twitter=TRUE,remove_numbers=TRUE,remove=stopwords(source="smart"))
outgroup.col <-brewer.pal(10, "BrBG")
textplot_wordcloud(outgroup.plot, min_count=16, color=outgroup.col)
title("Outgroup Wordcloud", col.main="grey14")
```

Let’s separate the training and test data-
```{r}
#separating Train and test data
text.train<-subset_M[1:7005,]
text.test<-subset_M[7006:nrow(subset_M),]

msg.dfm <- dfm(text.corpus, tolower = TRUE)  #generating document freq matrix
msg.dfm <- dfm_trim(msg.dfm, min_termfreqError = 5, min_docfreq = 1)  
msg.dfm <- dfm_weight(msg.dfm) 

head(msg.dfm)
```


#trining and testing data of dfm 
```{r}
msg.dfm.train<-msg.dfm[1:7005,]

msg.dfm.test<-msg.dfm[7006:nrow(subset_M),]

head(msg.dfm.train)
head(msg.dfm.test)
```

```{r}
#library(quanteda)
nb.classifier<-textmodel_nb(msg.dfm.train, text.train[,1])
nb.classifier
```




```{r}
predGroup <- predict(nb.classifier,msg.dfm.test)
```



#generating a confusion matrix

# use pred$nb.predicted to extract the class labels
```{r}
table(predicted=predGroup[["names"]],actual=text.test[,1])
mean(predGroup$names==text.test[,1])*100

```



##       actual
## predicted ham spam
##      ham  952    7
##      spam  16  140

16 text examples wrongly classified for ham and 7 examples wrongly classified for spam.
In the Confusion matrix,the diagonals are the correctly classified examples and the off-diagonals are the incorrectly classified text instances.

Now let’s calculate the accuracy of the model –

#acccuracy of the classifier on Test data
## [1] 97.93722

Now 98% accuracy is a good amount of accuracy on unseen random test data.










```{r}
subset_M$subject <- as.factor(subset_M$subject)
subset_M$structure <- as.factor(subset_M$structure)
```

```{r}
corpus <- Corpus(VectorSource(subset_M$text))
corpus
inspect(corpus[4:6])
```

```{r}
#In my code this block is split into substeps to make A/B testing easier.
corpus.clean <- corpus %>%
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords(kind="en")) %>%
  tm_map(stripWhitespace)
```
```{r}
corpus.dfm <- dfm(corpus.clean, tolower=TRUE)

# Inspect the dtm
#inspect(dtm[40:50, 10:15])
```

```{r}
dtm <- DocumentTermMatrix(corpus.clean)

# Inspect the dtm
inspect(dtm[40:50, 10:15])
```

```{r}
subset_M.train <- subset_M[1:7005,]
subset_M.test <- subset_M[7006:9340,]

dtm.train <- dtm[1:7005,]
dtm.test <- dtm[7006:9340,]

corpus.clean.train <- corpus.clean[1:7005]
corpus.clean.test <- corpus.clean[7006:9340]
```

```{r}
dim(dtm.train)
```

```{r}
fivefreq<- findFreqTerms(dtm.train, 5)
length((fivefreq))

dtm.train.nb <-DocumentTermMatrix(corpus.clean.train, control=list(dictionary=fivefreq))

dim(dtm.train.nb)

dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fivefreq))

dim(dtm.train.nb)
```

Here are the two blocks of code that switch between using a variation of the multinomial Naive Bayes algorithm known as binarized (boolean feature) Naive Bayes (from Dan Jurafsky) versus non-binarized. In this method, the term frequencies are replaced by Boolean presence/absence features. The logic behind this is that for sentiment classification, word occurrence matters more than word frequency. For my purposes, either might work, so both must be tested; put "r" in the opening brackets for whichever is needed.

```{r}
# Function to convert the word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}
```

```{}
# Function to KEEP word frequencies while still using the same variable names and code structure.
convert_count <- function(x) {
  y <- x
}
```

```{r}
# Apply the convert_count function to get final training and testing DTMs (whichever was chosen earlier)
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
```

```{r}
# Train the classifier. Not sure what system.time is needed for, except to show when block finishes.
system.time( classifier <- naiveBayes(trainNB, subset_M.train$subject, laplace = 1) )
system.time( classifier2 <- naiveBayes(trainNB, subset_M.train$structure, laplace = 1) )
```


##Testing the Predictions

```{r}
# Use the NB classifier we built to make predictions on the test set. Again, why use system.time?
system.time( pred <- predict(classifier, newdata=testNB) )
system.time( pred2 <- predict(classifier2, newdata=testNB) )
```

```{r}
# Create a truth table by tabulating the predicted class labels with the actual class labels 
line<-c("******************")
print("Subject")
table("Predictions"= pred,  "Actual" = subset_M.test$subject )
print(line)
print("Structure")
table("Predictions2"= pred2,  "Actual" = subset_M.test$structure )
```

```{r}
# Prepare the confusion matrix
conf.mat <- confusionMatrix(pred, subset_M.test$subject)
conf.mat
glimpse(line)

conf.mat$byClass
glimpse(line)

conf.mat$overall
glimpse(line)

conf.mat$overall['Accuracy']

glimpse(line)
glimpse(line)

# Prepare the confusion matrix
conf.mat2 <- confusionMatrix(pred2, subset_M.test$structure)
conf.mat2
glimpse(line)

conf.mat2$byClass
glimpse(line)

conf.mat2$overall
glimpse(line)

conf.mat2$overall['Accuracy']

```




###OPTIONAL Output
To make a table showing predicted categories for comments based on the Naive Bayes test, versus the actual label entered by a human observer, merge the two datasets into a single data frame. The full data frame will be too long to display; use "head" to check format, then output the full dataset to CSV for further review. By default the CSV file should be saved to the current working directory.

table("Predictions"= pred,  "Actual" = subset_M.test$subject )
print(line)
print("Structure")
table("Predictions2"= pred2,  "Actual" = subset_M.test$structure )

```{r}
predicted_category_subject <- data.frame(pred,subset_M.test$subject)
head(predicted_category_subject)
write.csv(predicted_category_subject, file = "PredictedCategorySubject_Data.csv")
```

```{r}
predicted_category_structure <- data.frame(pred,subset_M.test$structure)
head(predicted_category_structure)
write.csv(predicted_category_structure, file = "PredictedCategoryStructure_Data.csv")
```


***
START COMMENTS, OLD work notes
***







***
END COMMENTS
***

