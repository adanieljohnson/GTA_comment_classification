---
title: "Notes & Ideas"
bibliography: library.bib
biblio-style: apalike
---

```{r, child="_setup.Rmd"}
```

# Session 1 (Oct. 1, 2018)

####\  

# Session 2 (Oct. 22, 2018)

####\  

# Session 3 (Nov. 12, 2018)
I have 11,000 comments from TAs on student reports. Working out a process for bringing in the tidied data and structuring it for analysis is not finished, but is coming along. Some of the challenges and questions I have are outlined in Session 4.

While learning the coding, I have been thinking about how to approach the larger problem once data are in a reasonable form. I want to sort TA comments into topic groups more or less automatically, using pre-defined criteria. What are my options?

####\  

**Option: Latent Dirichlet Allocation**  

I found this summary, and a description of the approach at
[https://www.tidytextmining.com/topicmodeling.html]():  

>"In text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.

>"Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language."

My challenge is that LDA clusters items based on its own analysis. My groups are already sorted based on the **existing coding book.** The individual groups are already labeled. 

####\  

**Option: Comparison Modeling Based on Twitter Strategies**  

What if I treated each of the pre-defined sub-sets of comments I created using codebook analysis as if they were tweets from separate users? That approach is described here: 
	https://www.tidytextmining.com/twitter.html

Strategy would be:

*  Un-nest tokens to create a tidy frame and remove stop words using anti-join
*  Calculate word frequencies for each sub-type
*  Compare frequencies by calculating log-odds ratios

####\  

**Option: Comparison Modeling Via Text Classification**  

What if I treated each of the comments as text blocks where pre-defined words are more or less likely to be associated with the category. Jerid developed a Shiny app that does something similar:
https://francojc.github.io/2015/08/15/generating-annotated-text-in-shiny/

####\  

# Session 4 (Dec. 3, 2018)
Looking at Jerid's *Intro to Statistical Thinking* clarified some points in my approach. I definitely need to be thinking about this problem more in terms of unsupervised exploration. It did raise some follow-up questions: 

*  A training set with 80% of the data seems excessive. If there is a large dataset, is there any cost or benefit to splitting to 33/33/34%, or even 25% x 4? The 80/20 and 60/20/20% also seems out of balance, but there is probably logic I do not see. 
*  In the exploration phase, is re-sampling such a major concern?

**Separate note:** thank you many times over for the Regex101 link! Writing regex correctly has long been a pain for me! This greatly simplifies the process.  

####\  

####Confidentiality Problem
My initial dataset contains confidential information that needs to be de-identified. How do I convert the data so it can be displayed on a public site?

Most direct solution is to write bridge scripts. The versions that actually read my data in private repo. The public versions of these scripts would have 1+ empty placeholder spaces in the key commands that read in the data. Script documentation would explain how others can modify the bridge script file paths.

I assume that without correct system paths, outside users would not be able to access them. I can provide the de-identified data publicly, without exposing the confidential data. 

Any gaps or problems with this approach?

####\  

####Primary Data Keys For Structuring the Dataset?
These are values that are present already in the dataset.

*record.id*  

>Column is a semester ID and 5-digit number, like sp18.00001. This particular record.id indicates the data line is from Spring 2018 CSV file, and is line 00001 of the data table. This should be the unique primary key value for each record. Once assigned, it should not change.

*report.id*  

>Format is R\_nnn(...). The string is assigned to each entry made in Qualtrics. This particular string is assigned when a report is collected by our system. It is anonymous, and serves as a unique id for each REPORT document that we collect.

*student*  

>The email address of the SUBMITTING STUDENT. Unique to each student, and assigned by WFU. The email address connects multiple reports to a SINGLE STUDENT AUTHOR. It is NOT confidential though, so needs to be switched out. 

*ta*  

>The name of the TA assigning scores. Unique to instructor, and connects multiple reports scored by SINGLE TA. Again it is NOT confidential, so also needs to be switched out.

Consider either:

* Creating lists of non-repeating IDs with S\_nnn(...) = Student name, T\_nnn(...) = TA name. Then assign the names via a "lookup" style function
* Using a global names generator to build 25K list of randomly generated non-repeating first_last names. In practice, random names are easier to scan for visually.   

####\  

####Lagniappe
Maybe it is just me, but I've been struggling with some of the outside instructional materials becuase they use generic terms in ways that make code structure more confusing. 

For example, using either "text" or "word" as a variable_name is confusing if the same word is used as part of commands. An example I found of this is when I tried to use the "unnest_tokens" command. Look at the language below from one of the exercises; are the terms inside the parentheses variable_names, or modifiers for the command?

```
(text_tibble) %>% 
  unnest_tokens(word,text)
```

Just adding a numeral to show it is a variable name would help. For example, "text_1" clearly is not a command or modifier term. 
 
Maybe this goes back to developing a habit of adopting one approach (like snake_case, or never using "_" as the first character in a filename) and sticking to it.

####\  

####Miscellaneous To Do
* Need to review map, cast and bind commands again. Not sticking in my head.
* Try visualizing the 2-D array of the TA comments as a mosaic plot.

**Marking Lab Reports (random thoughts on a different project)**

Text files for processed lab reports already are structured so we can use them like corpus files with inline meta-data. All lab reports have this as embedded metadata up front:

* Number of lab report in semester: first
* This is a revision.
* Date submitted: 2018-11-06 12:17:40
* Author of this lab report is: *Student's NAME*
* ID: R_1et5dvwqfl2ZCX1
* Course number: 214
* TA: *TA's NAME*

Again, the main change we would need to make is to de-identify the student and TA. In theory I should be able to build some kind of direct replace functions for the TA-name and the Student-name. 




####\  

# Session 5 (Jan. 14, 2019)

####\  

# Session 6 (Feb. 4, 2019)

####\  

# Session 7 (Feb. 25, 2019)

####\  

# Session 8 (Mar. 18, 2019)

####\  

# References

####\  
