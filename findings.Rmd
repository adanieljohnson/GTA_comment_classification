---
title: "Findings"
output: html_document
bibliography: library.bib
biblio-style: apalike
---
```{r, child="_setup.Rmd"}
```

#Classifier Accuracy
*Updated 2/22/19*

The tables below show observed frequencies for 9,340 TA comments coded by hand, and frequencies for the same TA comments categorized using the optimized Naive Bayes classifier **without** pre-screening. The far right columns are overall subject frequencies, the bottom rows, overall structure frequencies.   

####\  

![*Comparison of frequency tables generated by hand-coding TA comments (top) versus predicted categories using optimized NB classifier (bottom). *](/Users/danjohnson/Dropbox/Coding_Tools/R_Environment/R_Projects/default_website/images/Result_of_NB_2D_mapping_Feb2019.png)

####\  

On average, the classifier is only 80-90% accurate for one subject sub-category or one specific comment. OVERALL though, the optimized NB classifier estimated relative frequencies of Subject subcategories very well; there was <1.5% difference in fractional frequencies between observed (hand-coded) and computationally assigned Subject sub-categories. 

Overall accuracy was considerably lower for Structure sub-categories. Specifically, the NB classifier over-assigned comments to the "Copy-editing" sub-category, and under-assigned comments to the "Specifics" sub-category. Most incorrectly classified comments belonged to the "Technical" Subject sub-category. This **suggest** there may be specific terms present in comments within this sub-category that are problematic, and warrant further exploration. 

####\  

##Data Story 2

#Overview
This notebook summarizes my progress, thoughts while building a working Ensemble method for categorizing TA comments.

####\  

##Replicate Cycles Test of Naive Bayes
One of my ideas was to see if I could refine Naive Bayes by running the same classifier with different randomized datasets, then putting individual outputs side by side and assigning final categories based on most frequent classification in the set. 

To test this I ran 5 Subject classifier replicates using the code in *Naive_bayes_V2r_2b.rmd*, with 5 different ```set.seed``` values. The exported CSV files were combined to get a set of data with up to 5 classifier assignments for the same comment.

Using multiple randomized NB training runs does not seem to help. First, it was rare for the  randomized runs to assign different incorrect classes. Second, when I compared 5 independently randomized runs by eye, the incorrectly categorized items were repeatedly put into the SAME incorrect group each time. Based on this, an Ensemble method using H-stacked categorical NB is unlikely to reduce overall errors in classification.

Looking at the combined data DID provide other helpful insights into the overall classification procedure though. Naive Bayes does not seem to do well with comments that:

*  Contain many or mostly punctuation symbols, such as:
    +  : not ,
    +  ?
    +  ??
    +  ???
*  Are whole numbers
    +  1
    +  2
    +  10
*  Are one word questions or comments
    +  because?
    +  "and"
*	 Are sentences enclosed in punctuation
*  Are made up entirely of STOP/common words (I am not as sure on this one)

####\  

##Overall Accuracy
I also looked at OVERALL accuracy of NB by recreating the 2-dimensional contingency table that I made when hand-coding, and comparing it side-by-side to NB classifications. From this I gained additional insights. 

OVERALL, the 2-D Naive Bayes model performed very well for classifying comment Subject. When I re-ran the full original dataset (9340 comments) through the prediction model, NB estimates for Subject were within 1.5% of my hand-scored assignments. Structure classification was not as robust. Other observations:

*  NB over-classified comments as "copyediting of technical issues" by 8-10%, and under-classified comments that should be marked as "specific technical issues" by almost same amount.
*  Comments that belonged to groups making up less than 1-2% of all comments were more likely to be mis-classified. 
*  Longer text comments seemed to be mis-classified more often. Follow-up methods to evaluate: 
    +  Split longer comments into shorter phrases
    +  Evaluate only the first 5-10 words of a comment

It appears that Subject categories are more terms-centered, while Structure is based more on the sentiment and sentence structure of a comment. This suggests that I need to incorporate a sentiment classifier or PoS tagger step.

####\  

##Mixed Ensemble Method
Based on the  seems like the most reasonable approach for maximum accuracy is going to be a mixed Ensemble method. The initial working model is to:

Add a pre-screening step prior to implementing NB:

*  Isolate "short-string" comments first. These are comments consisting of single words, two-word phrases, more punctuation than text, etc. These represent mostly pointers and copy-editing.
*  Look for 2-grams with high frequencies in one particular category. (Ex., "basic criteria" 2-gram identifies a Basic Criteria comment.) 

####\  


####\

#Tool or Analysis 2

####\

##Data Story 3

####\

##Data Story 4

####\



####\
