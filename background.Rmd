---
title: "Background"
bibliography: library.bib
biblio-style: apalike
---

```{r, child="_setup.Rmd"}
```

Undergraduate introductory STEM courses often include technical writing training. Too often it is treated as an incidental skill that each student develops "along the way." At other times the instructional strategy is  haphazardly designed, implemented, and executed. 

Adding to the learning challenge, faculty may not be the ones helping students develop writing skills. If technical writing training is part of a lab course, graduate teaching assistants or even advanced undergraduates may be who teaches technical writing. TAs may have under-developed writing skills themselves, lack pedagogical content knowledge needed to guide students effectively, or not have been explicitly trained to teach writing in a particular way. 

In 2014 we launched a report data intake process we call **SAWHET** that systematically collects writing and other data from student writers and graduate instructors. Using these data we are mapping key features of student writing, as well as connections and correlations between those features. These insights then inform how we teach technical writing and train graduate instructors.

Currently our structured database contains >5000 laboratory reports written by undergraduate students for introductory biology courses, all in standardized .docx and .txt formats. Additional data and metadata available for each report includes:

*  All comments that a TA made on each report
*  Context. Was it first or second report of the semester? Initial submission or revision? Course and topic prompt? Year in college of the author?
*  Instructor information. Which TA provided feedback? What score was assigned? What was rationale for score?

Our first goal is to better understand student and instructor behaviors within the technical writing instructional space. Our second and more practical goal is to develop techniques for teaching technical writing that:

*  Embody best practices from 40+ years of research on "writing in the disciplines;"
*  Emphasize student development through holistic coaching;
*  Shift student and instructor attention away from copy-correction towards larger writing issues; and
*  Can be implemented at scale by less experienced teaching assistants.

####\  
##Context & Prior Work
From prior studies (REFs), we know students make greater gains as writers if instructors avoid telling students what to correct ("copy editing"), and use a coaching-oriented approach instead. To that end, our program trains graduate TAs to use a **global issues first, specific issues later** approach to correcting undergraduate technical writing. We ask them to spend a shorter amount of time writing fewer comments that focus on broader issues. Ideally, TAs provide no more than 3-5 comments per text page, and the comments given should encourage student reflection. As students' writing improves, TAs can focus on smaller issues. Our bins-based report grading protocol reinforces this strategy.     

Our coaching-oriented approach is well-supported by prior research, but we are uncertain how accurately TAs implement the recommended strategy. We have repeatedly asked ourselves, **"what do TAs ACTUALLY spend most of their time and effort making comments about when they grade student writing?"** Self-reported TA effort is likely to be unreliable. What we need is a more direct measure of effort.

In 2018, we began trying to analyze TAs' comments from student papers directly. We extracted and tabulated all comments from approximately 500 pairs of reports. After initial data cleaning (removing duplicates, splitting comments addressing multiple independent writing issues, and removing comments lacking any context for judging intent), we had a working dataset of ~11,000 separate comments.

We next used a subset of 110 randomly selected comments to build an initial qualitative codebook for classifying comments based on subject and structure. This initial codebook was tested and revised using blocks of 1100 additional randomly selected comments until the categories and criteria stabilized. Using the final codebook, we then sorted all 11,000 comments into separate categories (see PAGE_LINK for complete codebook.)

TA comments were coded and sorted according to: 

* **Subject**: did the comment focus mainly on Basic criteria, Technical issues, Writing quality, Logic, or Other issues
* **Structure**: was the comment mainly Copy correction, General or Specific information, Pointing to another resource, or Holistic coaching for improvement?

Raw counts for each subject/structure pair were then converted to fractional frequencies. The final frequency table (excerpted below) provided us with a direct estimate of TA effort and emphasis.

![*Excerpted data from frequency table for hand-coded TA comments on student reports*](/Users/danjohnson/Dropbox/Coding_Tools/R_Environment/R_Projects/default_website/images/Hand_frequency_table.png)

####\  

##Central Problem
Analyzing TA comments this way is informative but coding by hand is unsustainable. Developing the codebook then rating the initial set of 11,000 comments required nearly 80 hours of investigator effort. Even with the codebook in hand, individual coders need training to achieve sufficient accuracy, and there is significant risk of "coding drift" over time. 

Given 10-12,000 comments are generated EACH SEMESTER, an automated classification method is needed.

####\  

##Work Plan
The work plan for this project is:

1.  Compile initial dataset and de-identify both TAs and students by replacing names with anonymous IDs.
2.  Import TA comments and relevant metadata from student reports into R as a "tidy" dataframe.
3.  Explore comment data structure, n-gram frequencies, etc., to identify potentially useful elements for feature engineering.
4.  Write an initial Naive Bayes workflow for supervised text classification.
5.  Assign TA comments to categories using the NB classifier, then compare outcomes from automated classification to hand-coded results.
6.  Using features (from Step #3) and comparison data from initial classification protocol (from Step #4) as guides, test iterations and optimize the classifier method. 
7.  Apply the optimized classifier to the original comment dataset, identify all comments that are being classified incorrectly, and look for potential patterns.
8.  Write a small set of rule- or REGEX pattern-based searches that can identify comments which are more likely to be classified incorrectly. 
9.  Append rules/pattern-based pre-screening to the optimized Naive Bayes classifier.
10.  Validate this vertical Ensemble Classifier by classifying a subset of ~2000 new TA comments extracted from a non-overlapping set of student lab reports. Compare automated results for new data set to hand-coded categories.

####\  
