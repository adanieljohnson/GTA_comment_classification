<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Primer of Text Classification</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 61px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 66px;
  margin-top: -66px;
}

.section h2 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h3 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h4 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h5 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h6 {
  padding-top: 66px;
  margin-top: -66px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Analyzing TA Comments in R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="notes.html">
    <span class="fa fa-sticky-note"></span>
     
    Notes
  </a>
</li>
<li>
  <a href="background.html">
    <span class="fa fa-history"></span>
     
    Background
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-search"></span>
     
    Lit Review
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="litreview.html">
        <span class="fa fa-search"></span>
         
        Overview
      </a>
    </li>
    <li>
      <a href="techwriting.html">
        <span class="fa fa-chalkboard"></span>
         
        Teaching Writing
      </a>
    </li>
    <li>
      <a href="futurefaculty.html">
        <span class="fa fa-chalkboard-teacher"></span>
         
        Training Future Faculty
      </a>
    </li>
    <li>
      <a href="aacr.html">
        <span class="fa fa-chart-bar"></span>
         
        Automated Text Analysis
      </a>
    </li>
    <li>
      <a href="Text_Classifier_Models.html">
        <span class="fa fa-sort-amount-down"></span>
         
        Intro to Text Classification
      </a>
    </li>
    <li>
      <a href="Topic_categorization.html">
        <span class="fa fa-code-branch"></span>
         
        Topic Modeling
      </a>
    </li>
    <li>
      <a href="Choosing_classifier.html">
        <span class="fa fa-hand-point-up"></span>
         
        Choosing a Classifier
      </a>
    </li>
    <li>
      <a href="Improving_classifier.html">
        <span class="fa fa-edit"></span>
         
        Evaluating Outcomes
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-puzzle-piece"></span>
     
    The Build
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="build.html">
        <span class="fa fa-puzzle-piece"></span>
         
        Overview
      </a>
    </li>
    <li>
      <a href="docxcommentextraction.html">
        <span class="fa fa-cut"></span>
         
        Comment Extraction
      </a>
    </li>
    <li>
      <a href="codebook.html">
        <span class="fa fa-book"></span>
         
        Classification Codebook
      </a>
    </li>
    <li>
      <a href="dataimport.html">
        <span class="fa fa-sign-in-alt"></span>
         
        About the Dataset
      </a>
    </li>
    <li>
      <a href="initial_exploration.html">
        <span class="fa fa-eye"></span>
         
        Initial Exploration
      </a>
    </li>
    <li>
      <a href="NB_2r2c.html">
        <span class="fa fa-map-marker-alt"></span>
         
        NB Classifier
      </a>
    </li>
    <li>
      <a href="Ensemble_Notes.html">
        <span class="fa fa-archive"></span>
         
        Ensemble Classifier
      </a>
    </li>
  </ul>
</li>
<li>
  <a href="initial_exploration.html">
    <span class="fa fa-question-circle"></span>
     
    Data Exploration
  </a>
</li>
<li>
  <a href="findings.html">
    <span class="fa fa-chart-bar"></span>
     
    Findings
  </a>
</li>
<li>
  <a href="sitemap.html">
    <span class="fa fa-shoe-prints"></span>
     
    Next Steps
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Primer of Text Classification</h1>
<h4 class="date"><em>2/21/2019</em></h4>

</div>


<p>This page provides general background on theory and approaches to text classification. Links below connect to sub-pages that describe more specific topics. Follow the links for details.</p>
<div id="section" class="section level4">
<h4>Â </h4>
</div>
<div id="basic-workflow" class="section level1">
<h1>Basic Workflow</h1>
<p>Text classification problems can be broken down into four overlapping sub-problems. Each piece may be more or less important to the overall problem, but all need to be considered.</p>
<ol style="list-style-type: decimal">
<li>Feature selection and engineering (<em>described below</em>)</li>
<li>Topic categorization and modeling (<a href="https://adanieljohnson.github.io/default_website/Topic_categorization.html" class="uri">https://adanieljohnson.github.io/default_website/Topic_categorization.html</a>)</li>
<li>Selecting an appropriate text classifier method (<a href="https://adanieljohnson.github.io/default_website/Choosing_classifier.html" class="uri">https://adanieljohnson.github.io/default_website/Choosing_classifier.html</a>)</li>
<li>Evaluating the outcomes, and improving the classifier (<a href="https://adanieljohnson.github.io/default_website/Improving_classifier.html" class="uri">https://adanieljohnson.github.io/default_website/Improving_classifier.html</a>)</li>
</ol>
<div id="section-1" class="section level4">
<h4>Â </h4>
</div>
</div>
<div id="feature-selection-and-engineering" class="section level1">
<h1>Feature Selection and Engineering</h1>
<p>This is the process of selecting, tabulating, calculating, or creating text-based features for subsequent evaluation. This includes:</p>
<ul>
<li>Identifying existing, directly observable text features within the dataset</li>
<li>Engineering additional features using established NLP methods</li>
<li>Reverse engineering relevant features based on an empirically defined feature set</li>
</ul>
<p>The features listed below are used routinely in natural language processing. Not all will be appropriate for all situations though; <strong>selecting, evaluating, comparing, and validating appropriate features for a classification task is an iterative process, not a one-time decision.</strong></p>
<ul>
<li>Count-based Vectors
<ul>
<li>Word Count â total number of words in the documents</li>
<li>Character Count â total number of characters in the documents</li>
<li>Punctuation Count â total number of punctuation marks in the documents</li>
<li>Upper Case Count â total number of upper case words in the documents</li>
<li>Title Word Count â total number of consecutive upper case (title) words in the documents</li>
</ul></li>
<li>Distribution and Frequency Calculations
<ul>
<li>Average word density of sentences</li>
<li>Average length of the words used in the documents</li>
<li>TF-IDF</li>
<li>Reading level estimates, readability (<em>described below</em>)</li>
</ul></li>
<li>Part of Speech Tagging and Frequency Distribution of Tags
<ul>
<li>Noun Count</li>
<li>Verb Count</li>
<li>Adjective Count</li>
<li>Adverb Count</li>
<li>Pronoun Count</li>
</ul></li>
<li>Word Embedding (described below)</li>
</ul>
<p>Additional features are available for longer texts such as multi-sentence paragraphs. These features require analysis of sentence structure, syntax, parts of speech patterns, etc.</p>
<ul>
<li>Word choices (vocabulary range)</li>
<li>Lexical diversity (<em>described below</em>)</li>
<li>Stance patterns - hedging, booster, generalization words or phrasing</li>
<li>Concision - comparisons of noun phrase density versus comment clause density</li>
<li>Cohesion - use of words that are additive (also), countering (however), specify or emphasize (even, both, especially), connecting (former, latter)</li>
<li>Civility and uncertainty - language that acknowledges limits of statement, critiques ideas not people, and makes fewer generalizations</li>
</ul>
<div id="section-2" class="section level4">
<h4>Â </h4>
</div>
</div>
<div id="some-feature-engineering-methods" class="section level1">
<h1>Some Feature Engineering Methods</h1>
<div id="lexical-diversity" class="section level2">
<h2>Lexical Diversity</h2>
<p>This is different from simple word frequencies The following explanation is an excerpt from âquanteda: Quantitative Analysis of Textual Dataâ, which describes how to use <code>textstat_lexdiv</code> to calculate lexical diversity. <a href="https://rdrr.io/cran/quanteda/man/textstat_lexdiv.html" class="uri">https://rdrr.io/cran/quanteda/man/textstat_lexdiv.html</a> <a href="http://quanteda.io/reference/textstat_lexdiv.html" class="uri">http://quanteda.io/reference/textstat_lexdiv.html</a></p>
<blockquote>
<p>Consider a speaker, who uses the term âallowâ multiple times throughout the speech, compared to an another speaker who uses terms âallow, concur, acquiesce, accede, and avowâ&quot; for the same word. The latter speech has more lexical diversity than the former. Lexical diversity is widely believed to be an important parameter to rate a document in terms of textual richness and effectiveness.</p>
</blockquote>
<blockquote>
<p>Lexical diversity, in simple terms, is a measurement of the breadth and variety of vocabulary used in a document. The different measures of lexical diversity are TTR, MSTTR, MATTR, C, R, CTTR, U, S, K, Maas, HD-D, MTLD, and MTLD-MA.</p>
</blockquote>
<p>koRpus package in R also provides functions to estimate lexical diversity or complexity. For more about this package, go to <a href="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781783551811/2/ch02lvl1sec16/lexical-diversity" class="uri">https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781783551811/2/ch02lvl1sec16/lexical-diversity</a></p>
<div id="section-3" class="section level4">
<h4>Â </h4>
</div>
</div>
<div id="readability-metrics" class="section level2">
<h2>Readability metrics</h2>
<p>These estimate the relative grade level or age of a typical reader who could comprehend 80+% (typically) of the text. Most readability metrics require longer blocks of text (100+ words) to achieve a stable score. They use some combination of word count, average syllables per word, and sentence length to estimate reading level.</p>
<p>The metrics differ in how appropriate they are for specific situations. Some were developed for general writing, others for specific school age groups, and still others for technical documents; the US Navy in particular developed a suite of metrics for evaluating their training documents.</p>
<p>The <code>readability library</code> (v0.1.1) is a collection of readability tools that utilize the <code>syllable</code> package for fast calculation of readability scores by grouping variables. It calculates Flesch Kincaid, Gunning Fog Index, Coleman Liau, SMOG, Automated Readability Index and an average of the 5 readability scores. These all are general scales; other scores may not be available in R packages.</p>
<div id="section-4" class="section level4">
<h4>Â </h4>
</div>
</div>
<div id="word-embedding" class="section level2">
<h2>Word Embedding</h2>
<p><strong>Word embedding</strong> is a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. The vectors then serve as engineered features for subsequent classifier functions.</p>
<p>Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension.</p>
<p>Methods to generate the mapping include neural networks, dimensionality reduction, probabilistic models, knowledge base methods, and explicit representation in terms of the context in which words appear. The technique of representing words as vectors has roots in the 1960s with the development of the vector space model for information retrieval.</p>
<p>Word embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur. Most new word embedding techniques rely on a neural network architecture instead of more traditional n-gram models and unsupervised learning.</p>
<p>One of the main limitations of word embeddings (and word vector space models in general) is that possible meanings of a word are conflated into a single representation (a single vector in the semantic space). Sense embeddings are a solution to this problem: individual meanings of words are represented as distinct vectors in the space.</p>
<p>Tools like <strong>Principal Component Analysis (PCA)</strong> and <strong>T-Distributed Stochastic Neighbour Embedding (t-SNE)</strong> are used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.</p>
<p>Software for training and using word embeddings includes Tomas Mikolovâs Word2vec, Stanford Universityâs GloVe, fastText, Gensim, AllenNLPâs Elmo, Indra and Deeplearning4j. Three of these tools are described further below.</p>
<div id="section-5" class="section level4">
<h4>Â </h4>
</div>
<div id="word2vec" class="section level4">
<h4>Word2vec</h4>
<p>Word2vec is a group of shallow, two-layer neural network models that produce word embeddings. The toolkit can train vector space models to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space so words that share common contexts in the corpus are located in close proximity to one another in the space.</p>
<p>The use of different model parameters and different corpus sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.</p>
<p>In models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.</p>
<p>Accuracy increases overall as the number of words used increases, and as the number of dimensions increases. Altszyler et al. <span class="citation">(Altszyler et al. 2017)</span> studied Word2vec performance in two semantic tests for different corpus size. They found that Word2vec has a steep learning curve, outperforming another word-embedding technique (LSA) when it is trained with medium to large corpus size (more than 10 million words). However, with a small training corpus LSA showed better performance. Additionally they show that the best parameter setting depends on the task and the training corpus. Nevertheless, for skip-gram models trained in medium size corpora, with 50 dimensions, a window size of 15 and 10 negative samples seems to be a good parameter setting.</p>
</div>
<div id="section-6" class="section level4">
<h4>Â </h4>
</div>
<div id="fasttext" class="section level4">
<h4>fastText</h4>
<p>fastText is a Python-based library created by Facebookâs AI Research (FAIR) lab for learning of word embeddings and text classification. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. fastText uses a neural network for word embedding.</p>
<p><a href="https://fasttext.cc/" class="uri">https://fasttext.cc/</a></p>
</div>
<div id="section-7" class="section level4">
<h4>Â </h4>
</div>
<div id="glove" class="section level4">
<h4>GloVe</h4>
<p>GloVe, short for Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. It is developed as an open-source project at Stanford. Pre-trained word vectors data are made available under the Public Domain Dedication and License v1.0.</p>
<p>GloVe uses Euclidean distance (or cosine similarity) between two word vector for measuring the linguistic or semantic similarity of the corresponding words. Sometimes, the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average humanâs vocabulary. For example, here are the closest words to the target word <strong>frog</strong>:</p>
<ul>
<li>frog</li>
<li>frogs</li>
<li>toad</li>
<li>litoria</li>
<li>leptodactylidae</li>
<li>rana</li>
<li>lizard</li>
<li>eleutherodactylus</li>
</ul>
<p>The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. This simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. For example, man may be regarded as similar to woman in that both words describe human beings; on the other hand, the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.</p>
<p>In order to capture in a quantitative way the nuance necessary to distinguish man from woman, it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words.</p>
<p><a href="https://nlp.stanford.edu/projects/glove/" class="uri">https://nlp.stanford.edu/projects/glove/</a></p>
</div>
<div id="section-8" class="section level4">
<h4>Â </h4>
<hr />
<p><em>Work Cited</em></p>
<div id="refs" class="references">
<div id="ref-Altszyler2017">
<p>Altszyler, E., S. Ribeiro, M. Sigman, and D. FernÃ¡ndez Slezak. 2017. âThe Interpretation of Dream Meaning: Resolving Ambiguity Using Latent Semantic Analysis in a Small Corpus of Text.â <em>Consciousness and Cognition</em> 56: 178â87.</p>
</div>
</div>
</div>
</div>
</div>

<p></p>
<hr>
<p>Copyright &copy; 2019 A. Daniel Johnson. All rights reserved.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
