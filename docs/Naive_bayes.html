<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Dan Johnson" />

<meta name="date" content="2018-12-21" />

<title>Naive Bayes Workflow</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 61px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 66px;
  margin-top: -66px;
}

.section h2 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h3 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h4 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h5 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h6 {
  padding-top: 66px;
  margin-top: -66px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Analyzing TA Comments in R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="course_notes.html">
    <span class="fa fa-leaf"></span>
     
    Course Notes
  </a>
</li>
<li>
  <a href="pre_work.html">
    <span class="fa fa-puzzle-piece"></span>
     
    Pre-Work
  </a>
</li>
<li>
  <a href="draft.html">
    <span class="fa fa-question-circle"></span>
     
    Draft
  </a>
</li>
<li>
  <a href="project.html">
    <span class="fa fa-archive"></span>
     
    Project
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Naive Bayes Workflow</h1>
<h4 class="author"><em>Dan Johnson</em></h4>
<h4 class="date"><em>12/21/2018</em></h4>

</div>


<div id="summary" class="section level2">
<h2>Summary</h2>
<p>This notebook documents a basic workflow for applying a naive Bayes model to terms in TA comments. I based it on a tutorial <a href="located%20here">https://rpubs.com/Billyhansen6/318412</a>. This is quoted from Hansen’s original tutorial: &gt;The objective of this project is to classify SMS messages as spam or ham (not spam). A Naive Bayes classifier approach will be used. This example is taken from chapter 4 of Machine Learning with R, Second Edition&quot;</p>
<blockquote>
<p>An example of the conditional probability that will be computed is as follows:</p>
</blockquote>
<p><span class="math display">\[P(Spam|Hospital) = P(Hospital|Spam)P(Spam)/P(Hospital)\]</span></p>
<blockquote>
<p>which is the formula for determining the probability that a message is spam given that it contains the word “Hospital” in the message.</p>
</blockquote>
<p>Lines ~30-205 are the baseline version of the workflow using single words. It correctly classifies comments in the top 4 sub-categories ~79% of the time. While that is close to minimum acceptable inter-rater reliability of two human readers, it is well below my goal of &gt;95% for use in routine automated analysis.</p>
<p>Lines ~208-290 describe an alternative method using n-grams. When I tested this approach using 2-grams, classification was slightly worse, at 68.3% correct.</p>
<p>Lines ~295-335 show how to use word clouds to visualize single word frequencies. I did not find this approach very useful.</p>
<p>To keep this code clean for now I will run various permutations in a scratch version, then summarize findings starting on line #345. In these subsequent iterations I will evaluate:</p>
<ul>
<li>Different text pre-processing strategies: stemmed versus unstemmed, numbers removed vs. not, etc.</li>
<li>Using n-grams rather than single words</li>
<li>Making pair-wise comparison versus comparing multiple groups simultaneously</li>
<li>Modifying the variables in the NB analysis itself.</li>
</ul>
<p>Finally, there are several things still missing or needed for this workflow:</p>
<ul>
<li>How to feed a text dataset in, and get a table of the predicted categories back out.</li>
<li>How to store the values from the Bayes prediction table so I can use them in calculations rather than type them in by hand.</li>
<li>How to randomize which comments are used for training, testing datasets.</li>
</ul>
<div id="section" class="section level4">
<h4> </h4>
</div>
</div>
<div id="initial-setup" class="section level2">
<h2>Initial Setup</h2>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ───────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 2.2.1     ✔ purrr   0.2.4
## ✔ tibble  1.4.2     ✔ dplyr   0.7.4
## ✔ tidyr   0.8.0     ✔ stringr 1.3.0
## ✔ readr   1.1.1     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ──────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(tidytext)
library(tidyr)
library(tm)</code></pre>
<pre><code>## Loading required package: NLP</code></pre>
<pre><code>## 
## Attaching package: &#39;NLP&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     annotate</code></pre>
<pre class="r"><code>library(SnowballC)
library(wordcloud)</code></pre>
<pre><code>## Loading required package: RColorBrewer</code></pre>
<pre class="r"><code>library(e1071)
library(gmodels)</code></pre>
<div id="section-1" class="section level4">
<h4> </h4>
</div>
</div>
<div id="baseline-version-1.0-of-naive-bayes-nb-protocol" class="section level2">
<h2>Baseline Version 1.0 of Naive Bayes (NB) Protocol</h2>
<div id="initial-data-input" class="section level3">
<h3>Initial Data Input</h3>
<pre class="r"><code>#Read in TA comments from CSV file.
base_data &lt;- read_csv(file=&#39;data/coded_full_comments_dataset_Spring18anon.csv&#39;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_character(),
##   sort = col_integer(),
##   course = col_integer(),
##   Rank = col_integer()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<pre class="r"><code>#Select rows representing the sub-groups to compare. 
comments_subset &lt;- filter(base_data,code.subject==&quot;1. Basic Criteria&quot;|code.subject==&quot;2. Writing Quality&quot;|code.subject==&quot;3. Technical and Scientific&quot;|code.subject==&quot;4. Logic and Thinking&quot;)

#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
comments_raw &lt;- comments_subset %&gt;% select(23,22)

#Rename the columns.
names(comments_raw)[1] &lt;- &quot;type&quot;
names(comments_raw)[2] &lt;- &quot;text&quot;

#Simplify coding terms
comments_raw[,1] &lt;- ifelse(comments_raw[,1] == &quot;1. Basic Criteria&quot;,&quot;1_Basic&quot;, ifelse(comments_raw[,1] == &quot;2. Writing Quality&quot;,&quot;2_Writing&quot;, ifelse(comments_raw[,1] == &quot;3. Technical and Scientific&quot;,&quot;3_Technical&quot;, ifelse(comments_raw[,1] == &quot;4. Logic and Thinking&quot;,&quot;4_Logic&quot;,99))))

#Change &quot;type&quot; element from character to a factor for analysis.
comments_raw$type &lt;- factor(comments_raw$type)
str(comments_raw$type)</code></pre>
<pre><code>##  Factor w/ 4 levels &quot;1_Basic&quot;,&quot;2_Writing&quot;,..: 3 2 3 3 3 3 4 4 4 3 ...</code></pre>
<pre class="r"><code>table(comments_raw$type)</code></pre>
<pre><code>## 
##     1_Basic   2_Writing 3_Technical     4_Logic 
##         211        2578        5409        1142</code></pre>
<p>Data set must be converted to a volative corpus using “tm” library then transformed.</p>
<pre class="r"><code>#Create the volatile coprus that contains the &quot;text&quot; vector from data frame.
comments_corpus &lt;- VCorpus(VectorSource(comments_raw$text))
print(comments_corpus)</code></pre>
<pre><code>## &lt;&lt;VCorpus&gt;&gt;
## Metadata:  corpus specific: 0, document level (indexed): 0
## Content:  documents: 9340</code></pre>
<pre class="r"><code>#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpus[1:3])</code></pre>
<pre><code>## &lt;&lt;VCorpus&gt;&gt;
## Metadata:  corpus specific: 0, document level (indexed): 0
## Content:  documents: 3
## 
## [[1]]
## &lt;&lt;PlainTextDocument&gt;&gt;
## Metadata:  7
## Content:  chars: 15
## 
## [[2]]
## &lt;&lt;PlainTextDocument&gt;&gt;
## Metadata:  7
## Content:  chars: 31
## 
## [[3]]
## &lt;&lt;PlainTextDocument&gt;&gt;
## Metadata:  7
## Content:  chars: 70</code></pre>
<pre class="r"><code>#Use &quot;as.character&quot; function to see what a message looks like.
as.character(comments_corpus[[3]])</code></pre>
<pre><code>## [1] &quot;Is this the most concise way to communicate no significant difference?&quot;</code></pre>
<p>The OPTIONAL data transforms using “tm_map”&quot; and “content_transformer” functions. Switch between “{r}” and “{}” to turn a transformation block off or on.</p>
<pre class="r"><code>#Convert to all lower case letters.
comments_corpus_clean &lt;- tm_map(comments_corpus, content_transformer(tolower))</code></pre>
<pre class="r"><code>#Remove numbers. 
comments_corpus_clean &lt;- tm_map(comments_corpus_clean, removeNumbers)</code></pre>
<pre class="r"><code>#Stopword removal. 
comments_corpus_clean &lt;- tm_map(comments_corpus_clean, removeWords, stopwords())</code></pre>
<pre class="r"><code>#Remove punctuation using the &quot;removePunctuation&quot; function. 
#This step removes evidence of questions, so may remove data.
comments_corpus_clean &lt;- tm_map(comments_corpus_clean, removePunctuation)</code></pre>
<pre class="r"><code>#Stemming the text data to strip the suffix from words.  
comments_corpus_clean &lt;- tm_map(comments_corpus_clean, stemDocument)</code></pre>
<p>Final prep and data check.</p>
<pre class="r"><code>#Final step removes white space from the document. This is NOT optional.
comments_corpus_clean &lt;- tm_map(comments_corpus_clean, stripWhitespace)

#Look at an example of data.
as.character((comments_corpus_clean[[3]]))</code></pre>
<pre><code>## [1] &quot;concis way communic signific differ&quot;</code></pre>
<div id="section-2" class="section level4">
<h4> </h4>
</div>
</div>
<div id="basic-tokenizer" class="section level3">
<h3>Basic Tokenizer</h3>
<p>This version uses the “DocumentTermMatrix” function to creates a matrix in which the rows indicate individual documents (individual TA comments) and the columns indicate presence or absence of the word that is the header for the column. Be careful not to confuse it with “TermDocumentMatrix”, which transposes the matrix.</p>
<p>“DocumentTermMaxtrix” function also can perform all of the text prep above in one command.</p>
<pre class="r"><code>comments_corpus_1gram &lt;- DocumentTermMatrix(comments_corpus_clean)
comments_corpus_1gram_tidy &lt;- tidy(comments_corpus_1gram)
inspect(comments_corpus_1gram)</code></pre>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 9340, terms: 3340)&gt;&gt;
## Non-/sparse entries: 61737/31133863
## Sparsity           : 100%
## Maximal term length: 66
## Weighting          : term frequency (tf)
## Sample             :
##       Terms
## Docs   can data figur hypothesi includ just need result sentenc use
##   1499   0    1     0         0      1    0    1      0       1   1
##   2986   0    0     1         0      3    1    0      3       0   0
##   3187   1    1     2         0      0    0    0      1       0   0
##   4079   0    1     2         0      0    0    1      0       0   0
##   6239   1    2     4         0      0    0    1      0       0   1
##   6288   1    1     0         0      3    0    1      0       0   1
##   7974   3    0     0         0      0    0    0      0       0   0
##   8990   1    0     0         1      1    1    0      0       1   2
##   9149   1    2     0         0      0    1    0      0       1   1
##   9156   0    0     0         1      1    0    0      0       0   0</code></pre>
<div id="section-3" class="section level4">
<h4> </h4>
</div>
</div>
<div id="data-preparation" class="section level3">
<h3>Data Preparation</h3>
<p>Split data into 75% training and 25% testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data.</p>
<pre class="r"><code>.75 * 9340 #Number of documents/rows in the dataset; product is #rows for training</code></pre>
<pre><code>## [1] 7005</code></pre>
<pre class="r"><code>.25 * 9340 #Product is #rows for testing set</code></pre>
<pre><code>## [1] 2335</code></pre>
<p>This code assumes comments are random. <strong>Probably want to try randomizing them.</strong></p>
<pre class="r"><code>comments_dtm_train &lt;- comments_corpus_1gram[1:7005, ]
comments_dtm_test &lt;- comments_corpus_1gram[7006:9340, ]</code></pre>
<p>Save vectors labeling rows in the training and testing vectors</p>
<pre class="r"><code>comments_train_labels &lt;- comments_raw[1:7005, ]$type
comments_test_labels &lt;- comments_raw[7006:9340,]$type</code></pre>
<p>Make sure that the proportion of each sub-category is similar in the training and testing data set.</p>
<pre class="r"><code>prop.table(table(comments_train_labels))</code></pre>
<pre><code>## comments_train_labels
##     1_Basic   2_Writing 3_Technical     4_Logic 
##  0.02212705  0.27908637  0.57787295  0.12091363</code></pre>
<pre class="r"><code>prop.table(table(comments_test_labels))</code></pre>
<pre><code>## comments_test_labels
##     1_Basic   2_Writing 3_Technical     4_Logic 
##  0.02398287  0.26680942  0.58286938  0.12633833</code></pre>
<div id="section-4" class="section level4">
<h4> </h4>
</div>
</div>
<div id="preparation-for-naive-bayes" class="section level3">
<h3>Preparation for Naive Bayes</h3>
<p>Remove words from the matrix that appear less than 5 times.</p>
<pre class="r"><code>comments_freq_words &lt;- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)</code></pre>
<pre><code>##  chr [1:1043] &quot;’ll&quot; &quot;’re&quot; &quot;’ve&quot; &quot;“correct”&quot; &quot;“figur&quot; &quot;“group&quot; ...</code></pre>
<p>Limit our Document Term Matrix to only include words in the sms_freq_vector. We want all the rows, but we want to limit the column to these words in the frequency vector.</p>
<pre class="r"><code>comments_dtm_freq_train &lt;- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test &lt;- comments_dtm_test[ , comments_freq_words]</code></pre>
<p>The naive bayes classifier works with categorical features, so we need to convert the matrix to “yes” and “no” categorical variables. To do this we’ll build a convert_counts function and apply it to our data.</p>
<pre class="r"><code>convert_counts2 &lt;- function(x) {
  x &lt;- ifelse(x &gt; 0, &quot;Yes&quot;, &quot;No&quot;)
}</code></pre>
<p>This replaces values greater than 0 with yes, and values not greater than 0 with no. Let’s apply it to our data.</p>
<pre class="r"><code>comments_train &lt;- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test &lt;- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)</code></pre>
<p>The resulting matrixes will be character type, with cells indicating “yes” or “no” if the word represented by the column appears in the message represented by the row.</p>
<div id="section-5" class="section level4">
<h4> </h4>
</div>
</div>
<div id="train-model-predict-evaluate" class="section level3">
<h3>Train Model, Predict, Evaluate</h3>
<p>Use the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group ONE, TWO, THREE, or FOUR. Evaluate the prediction with the actual data using a crosstable from the gmodels package.</p>
<pre class="r"><code>comments_classifier &lt;- naiveBayes(comments_train, comments_train_labels)
comments_test_pred &lt;- predict(comments_classifier, comments_test)
CrossTable(comments_test_pred, comments_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c(&#39;predicted&#39;, &#39;actual&#39;))</code></pre>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |           N / Row Total |
## |           N / Col Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  2335 
## 
##  
##              | actual 
##    predicted |     1_Basic |   2_Writing | 3_Technical |     4_Logic |   Row Total | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##      1_Basic |          42 |          10 |          23 |           6 |          81 | 
##              |       0.519 |       0.123 |       0.284 |       0.074 |       0.035 | 
##              |       0.750 |       0.016 |       0.017 |       0.020 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##    2_Writing |           5 |         449 |          95 |          60 |         609 | 
##              |       0.008 |       0.737 |       0.156 |       0.099 |       0.261 | 
##              |       0.089 |       0.721 |       0.070 |       0.203 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##  3_Technical |           8 |         106 |        1185 |          43 |        1342 | 
##              |       0.006 |       0.079 |       0.883 |       0.032 |       0.575 | 
##              |       0.143 |       0.170 |       0.871 |       0.146 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##      4_Logic |           1 |          58 |          58 |         186 |         303 | 
##              |       0.003 |       0.191 |       0.191 |       0.614 |       0.130 | 
##              |       0.018 |       0.093 |       0.043 |       0.631 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
## Column Total |          56 |         623 |        1361 |         295 |        2335 | 
##              |       0.024 |       0.267 |       0.583 |       0.126 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
## 
## </code></pre>
<p>Calculate overall correct prediction rate.</p>
<pre class="r"><code>(42+454+1186+185)/23.35</code></pre>
<pre><code>## [1] 79.95717</code></pre>
<hr />
</div>
</div>
<div id="modified-naive-bayes-nb-method-using-n-grams" class="section level2">
<h2>Modified Naive Bayes (NB) Method Using N-grams</h2>
<p>The tokenizer command block above only works for single words. The block below uses tm’s NLP commands to generate either 1-, 2-, or 3-grams with the same command. The only change needed is “#” in “(words(x), #)” below. For simplicity I left off all of the preparatory code up leading up to the final cleaned corpus.</p>
<pre class="r"><code>NLP_Tokenizer &lt;- function(x) {
      unlist(lapply(ngrams(words(x), 2), paste, collapse = &quot; &quot;), use.names = FALSE)
}

comments_dtm_2gram &lt;- DocumentTermMatrix(comments_corpus_clean, control=list(tokenize = NLP_Tokenizer))
comments_dtm_2gram_tidy &lt;- tidy(comments_dtm_2gram)
inspect(comments_dtm_2gram)</code></pre>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 9340, terms: 30478)&gt;&gt;
## Non-/sparse entries: 58317/284606203
## Sparsity           : 100%
## Maximal term length: 71
## Weighting          : term frequency (tf)
## Sample             :
##       Terms
## Docs   citat format don’t need error bar figur caption make sure
##   1499            0          1         0             0         0
##   2986            0          0         0             0         0
##   3187            0          0         0             0         0
##   4079            0          0         2             0         0
##   6239            0          0         0             0         0
##   6288            0          0         0             0         0
##   7974            0          0         0             0         0
##   8990            0          0         0             0         0
##   9149            0          0         0             0         0
##   9156            0          0         0             0         0
##       Terms
## Docs   method section primari literatur raw data standard deviat
##   1499              0                 0        0               0
##   2986              0                 0        0               0
##   3187              0                 0        0               0
##   4079              0                 0        0               1
##   6239              0                 0        0               0
##   6288              0                 0        0               0
##   7974              0                 0        0               0
##   8990              0                 0        0               0
##   9149              1                 0        0               0
##   9156              0                 0        0               0
##       Terms
## Docs   statist test
##   1499            0
##   2986            0
##   3187            0
##   4079            0
##   6239            0
##   6288            0
##   7974            0
##   8990            0
##   9149            2
##   9156            0</code></pre>
<div id="section-6" class="section level4">
<h4> </h4>
</div>
<div id="data-preparation-1" class="section level3">
<h3>Data Preparation</h3>
<p>Split data into 75% training and 25% testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data.</p>
<pre class="r"><code>.75 * 9340 #Number of rows in the dataset; product is #rows for training</code></pre>
<pre><code>## [1] 7005</code></pre>
<pre class="r"><code>.25 * 9340 #Product is #rows for testing set</code></pre>
<pre><code>## [1] 2335</code></pre>
<p>This code assumes comments are random. <strong>Probably want to try randomizing them.</strong></p>
<pre class="r"><code>comments_dtm_train &lt;- comments_dtm_2gram[1:7005, ]
comments_dtm_test &lt;- comments_dtm_2gram[7006:9340, ]</code></pre>
<p>Save vectors labeling rows in the training and testing vectors</p>
<pre class="r"><code>comments_train_labels &lt;- comments_raw[1:7005, ]$type
comments_test_labels &lt;- comments_raw[7006:9340,]$type</code></pre>
<p>Make sure that the proportion of each sub-category is similar in the training and testing data set.</p>
<pre class="r"><code>prop.table(table(comments_train_labels))</code></pre>
<pre><code>## comments_train_labels
##     1_Basic   2_Writing 3_Technical     4_Logic 
##  0.02212705  0.27908637  0.57787295  0.12091363</code></pre>
<pre class="r"><code>prop.table(table(comments_test_labels))</code></pre>
<pre><code>## comments_test_labels
##     1_Basic   2_Writing 3_Technical     4_Logic 
##  0.02398287  0.26680942  0.58286938  0.12633833</code></pre>
<div id="section-7" class="section level4">
<h4> </h4>
</div>
</div>
<div id="preparation-for-naive-bayes-1" class="section level3">
<h3>Preparation for Naive Bayes</h3>
<p>Remove words from the matrix that appear less than 5 times.</p>
<pre class="r"><code>comments_freq_words &lt;- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)</code></pre>
<pre><code>##  chr [1:1323] &quot;– ’s&quot; &quot;’m assum&quot; &quot;’m confus&quot; &quot;’m sure&quot; &quot;’re go&quot; ...</code></pre>
<p>Limit our Document Term Matrix to only include words in the sms_freq_vector. We want all the rows, but we want to limit the column to these words in the frequency vector.</p>
<pre class="r"><code>comments_dtm_freq_train &lt;- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test &lt;- comments_dtm_test[ , comments_freq_words]</code></pre>
<p>The naive bayes classifier works with categorical features, so we need to convert the matrix to “yes” and “no” categorical variables. To do this we’ll build a convert_counts function and apply it to our data.</p>
<pre class="r"><code>convert_counts2 &lt;- function(x) {
  x &lt;- ifelse(x &gt; 0, &quot;Yes&quot;, &quot;No&quot;)
}</code></pre>
<p>This replaces values greater than 0 with yes, and values not greater than 0 with no. Let’s apply it to our data.</p>
<pre class="r"><code>comments_train &lt;- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test &lt;- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)</code></pre>
<p>The resulting matrixes will be character type, with cells indicating “yes” or “no” if the word represented by the column appears in the message represented by the row.</p>
<div id="section-8" class="section level4">
<h4> </h4>
</div>
</div>
<div id="train-model-predict-evaluate-1" class="section level3">
<h3>Train Model, Predict, Evaluate</h3>
<p>Use the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group ONE, TWO, THREE, or FOUR. Evaluate the prediction with the actual data using a crosstable from the gmodels package.</p>
<pre class="r"><code>comments_classifier &lt;- naiveBayes(comments_train, comments_train_labels)
comments_test_pred &lt;- predict(comments_classifier, comments_test)
CrossTable(comments_test_pred, comments_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c(&#39;predicted&#39;, &#39;actual&#39;))</code></pre>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |           N / Row Total |
## |           N / Col Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  2335 
## 
##  
##              | actual 
##    predicted |     1_Basic |   2_Writing | 3_Technical |     4_Logic |   Row Total | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##      1_Basic |          37 |           5 |          17 |           1 |          60 | 
##              |       0.617 |       0.083 |       0.283 |       0.017 |       0.026 | 
##              |       0.661 |       0.008 |       0.012 |       0.003 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##    2_Writing |           3 |         239 |          73 |          41 |         356 | 
##              |       0.008 |       0.671 |       0.205 |       0.115 |       0.152 | 
##              |       0.054 |       0.384 |       0.054 |       0.139 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##  3_Technical |          14 |         327 |        1234 |         142 |        1717 | 
##              |       0.008 |       0.190 |       0.719 |       0.083 |       0.735 | 
##              |       0.250 |       0.525 |       0.907 |       0.481 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
##      4_Logic |           2 |          52 |          37 |         111 |         202 | 
##              |       0.010 |       0.257 |       0.183 |       0.550 |       0.087 | 
##              |       0.036 |       0.083 |       0.027 |       0.376 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
## Column Total |          56 |         623 |        1361 |         295 |        2335 | 
##              |       0.024 |       0.267 |       0.583 |       0.126 |             | 
## -------------|-------------|-------------|-------------|-------------|-------------|
## 
## </code></pre>
<pre class="r"><code>(34+214+1254+94)/23.35</code></pre>
<pre><code>## [1] 68.35118</code></pre>
<hr />
<hr />
<div id="section-9" class="section level4">
<h4> </h4>
</div>
</div>
</div>
<div id="visualization" class="section level2">
<h2>Visualization</h2>
<p>This code generates a wordcloud of the frequency of single words in the dataset using the package “wordcloud”.</p>
<pre class="r"><code>wordcloud(comments_corpus_clean, max.words = 50, random.order = FALSE)</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Compare wordclouds between 3 groups.</p>
<pre class="r"><code>ONE &lt;- subset(comments_raw, type == &quot;1_Basic&quot;)
TWO &lt;- subset(comments_raw, type == &quot;2_Writing&quot;)
THREE &lt;- subset(comments_raw, type == &quot;3_Technical&quot;)
FOUR &lt;- subset(comments_raw, type == &quot;4_Logic&quot;)

wordcloud(ONE$text, max.words = 50, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,
## tm::stopwords())): transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents

## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-33-2.png" width="672" /></p>
<pre class="r"><code>wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents

## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-33-3.png" width="672" /></p>
<pre class="r"><code>wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents

## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-33-4.png" width="672" /></p>
<p>Show the 5 most frequent words in the data:</p>
<pre class="r"><code>comment_sack &lt;- TermDocumentMatrix(comments_corpus_clean)
comment_pack &lt;- as.matrix(comment_sack)
comment_snack &lt;- sort(rowSums(comment_pack), decreasing = TRUE)
comment_hack &lt;- data.frame(word = names(comment_snack), freq=comment_snack)
head(comment_hack, 10)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["word"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["freq"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"need","2":"1402","_rn_":"need"},{"1":"result","2":"1071","_rn_":"result"},{"1":"use","2":"901","_rn_":"use"},{"1":"figur","2":"855","_rn_":"figur"},{"1":"includ","2":"697","_rn_":"includ"},{"1":"can","2":"629","_rn_":"can"},{"1":"hypothesi","2":"599","_rn_":"hypothesi"},{"1":"just","2":"586","_rn_":"just"},{"1":"data","2":"546","_rn_":"data"},{"1":"sentenc","2":"542","_rn_":"sentenc"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>And the 5 most frequent words from each class:</p>
<pre class="r"><code>wordcloud(ONE$text, max.words = 10, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,
## tm::stopwords())): transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<pre class="r"><code>wordcloud(TWO$text, max.words = 10, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents

## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-35-2.png" width="672" /></p>
<pre class="r"><code>wordcloud(THREE$text, max.words = 10, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents

## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-35-3.png" width="672" /></p>
<pre class="r"><code>wordcloud(FOUR$text, max.words = 10, scale = c(3, 0.5))</code></pre>
<pre><code>## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents

## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):
## transformation drops documents</code></pre>
<p><img src="Naive_bayes_files/figure-html/unnamed-chunk-35-4.png" width="672" /></p>
<div id="section-10" class="section level4">
<h4> </h4>
</div>
</div>
<div id="permutation-tests-of-naive-bayes-nb-method" class="section level2">
<h2>Permutation Tests of Naive Bayes (NB) Method</h2>
<p>Version 1.0 correctly identifies comment subcategories ~79% of the time when using single words, and 68.3% of the time when using 2-grams. Permutations to test:</p>
<ul>
<li>Compare 2-grams, 3-grams rather than single words</li>
<li>Make pair-wise comparisons versus comparing multiple groups simultaneously.
<ul>
<li>Ex.: Basic vs. Not Basic (1_Basic vs. not 1_Basic)</li>
</ul></li>
<li>Modify the variables within the NB analysis itself.
<ul>
<li>Laplace constant (Y/N), Cutoff value for words removed from list.</li>
</ul></li>
<li>Do/Do not stem the terms
<ul>
<li>This may remove evidence of tense</li>
</ul></li>
<li>Remove/Leave stopwords</li>
<li>Remove/Leave capitalization</li>
<li>Remove/Leave numbers
<ul>
<li>This may remove evidence of technical descriptors</li>
</ul></li>
<li>Remove/Leave punctuation
<ul>
<li>This may remove evidence of questions vs statements</li>
</ul></li>
</ul>
</div>

<p>Copyright &copy; 2018 A. Daniel Johnson. All rights reserved.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
