---
title: "Naive Bayes Workflow Ver 2.0.yn"
author: "Dan Johnson"
date: "1/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Summary
This is **Version 2.0.yn** of a basic workflow for classifying TA comments using a naive Bayes model. It is optimized for testing pair-wise assignment to categories (Yes/No decision) beginning with the most frequently encountered:

*  3_technical for the Subject category
*  4_specific for the Structure category

The main features and permutations to be evaluated are:

*  Stepwise paired classification versus comparing multiple groups simultaneously

Additional working notes are located at the end of this page.

####\  

##Initial Setup
Calling required libraries.

```{r}
library(tidyverse)
library(tidytext)
library(tidyr)
library(tm)
library(e1071)
library(gmodels)
```

####\  

##Version 2.0.YN of Naive Bayes (NB) Protocol
###Initial Data Input
```{r}
#Read in TA comments from CSV file.
base_dataVerYN <- read_csv(file='data/coded_full_comments_dataset_Spring18anonVERB.csv')
```

###Extract, Organize Data Subset
Extract comments and classification data subset
```{r}
#Select rows representing the sub-groups to compare. 
comments_subsetVerYN <- filter(base_dataVerYN,code.subject=="1_basic"|code.subject=="2_writing"|code.subject=="3_technical"|code.subject=="4_logic")

#Reduce larger dataframe to 3 required columns of data, and put columns in order needed.
comments_rawVerYN <- comments_subsetVerYN %>% select(23,24,22)

#Rename the columns.
names(comments_rawVerYN)[1] <- "subject"
names(comments_rawVerYN)[2] <- "structure"
names(comments_rawVerYN)[3] <- "text"

#This block uses 'forcats' to recode the Subject and Structure factors as either ingroup or outgroup.
#Changing this block is how I change and use other Subject and Structure factors as the ingroup.
#Looking ahead, could use this block and a variable name to simplify coding (NO, does not work).

comments_rawVerYN_V <- comments_rawVerYN %>% 
  mutate(subject = fct_recode(subject, ingroup = "3_technical", outgroup="1_basic", outgroup="2_writing", outgroup="4_logic")) %>% 
  mutate(structure = fct_recode(structure, ingroup = "4_specific", outgroup="1_pointer",outgroup="2_copyedit",outgroup="3_general",outgroup="6_holistic",outgroup="7_idiomatic",outgroup="8_nobasis" ))
```

***
POINT OF CONCERN ABOUT CODING
***
Doing multiple comments for a larger group like Structure is tedious. This block of code:
```
comments_rawVerYN_V <- comments_rawVerYN %>% 
    mutate(structure = fct_recode(structure, ingroup = "4_specific", outgroup="1_pointer",outgroup="2_copyedit",outgroup="3_general",outgroup="6_holistic",outgroup="7_idiomatic",outgroup="8_nobasis" ))
```
Has two issues. 
1. Each item being recoded as outgroup has to be typed in individually. If I try to enter multiple items in a list (either by embedding the list in a variable, or by concatenating the items to all be recoded as 'outgroup'), I get an error message saying you cannot apply fct_recode to more than one pair of items. Seems to be a lot of code to write to recode several variables to the same name. 

2. This block was suggested, but is for writing the summary list of who gets COUNTED HOW; it does not affect the actual coding. 
```
partyid2 <- fct_collapse(gss_cat$partyid,
  missing = c("No answer", "Don't know"),
  other = "Other party",
  rep = c("Strong republican", "Not str republican"),
  ind = c("Ind,near rep", "Independent", "Ind,near dem"),
  dem = c("Not str democrat", "Strong democrat")
)
```
I do not want to collapse the COUNTS values together, I want to recode multiple variables simultaneously. I want to choose ALL of the NON-INGROUP items with one command and not add them individually. This is pseudo-code describing what I want to do:

```{}
comments_rawVerYN %>%
  mutate(subject = fct_recode(subject, ingroup = "3_technical", outgroup= ANYTHING_BUT "3_technical"))
```

3. Other suggestions I found:

```
https://community.rstudio.com/t/quickly-recoding-large-ish-vectors-in-a-tibble/2447

# object to decode and the key

tbl <- tibble(CODE = sample(letters[1:3], 1e+06, replace = TRUE))

key <- tribble(~CODE,   ~FRUIT, 
                 "a",  "apple", 
                 "b", "banana", 
                 "c",  "cherry"
               )

# Alternative One: dplyr::recode (what I used above basically)

mutate(tbl, FRUIT = recode(CODE, a = "apple", b = "banana", c = "cherry")) 
#> # A tibble: 1,000,000 x 2
#>     CODE  FRUIT
#>    <chr>  <chr>
#>  1     c cherry
#>  2     b banana
#>  3     a  apple
#>  4     b banana
#>  5     c cherry
#>  6     c cherry
#>  7     b banana
#>  8     c cherry
#>  9     a  apple
#> 10     a  apple
#> # ... with 999,990 more rows

# Alternative Two: named vector

# create a named vector from the key object and use it to recode
named_vector <- 
    key %>% 
    pmap(~set_names(..2, ..1)) %>% 
    unlist()

mutate(tbl, FRUIT = named_vector[CODE])
#> # A tibble: 1,000,000 x 2
#>     CODE  FRUIT
#>    <chr>  <chr>
#>  1     c cherry
#>  2     b banana
#>  3     a  apple
#>  4     b banana
#>  5     c cherry
#>  6     c cherry
#>  7     b banana
#>  8     c cherry
#>  9     a  apple
#> 10     a  apple
#> # ... with 999,990 more rows


left_join(tibblename, key)

benchmark(left_join(tbl, key, by = "CODE"), 
          columns = c("replications", "elapsed", "relative", "user.self", "sys.self"))



tbl <- tibble(CODE = sample(letters[1:3], 1e+06, replace = TRUE))


rev <- tbl %>%
  mutate(type =
           case_when(
             CODE == "a" ~ "apple",
             CODE == "b" ~ "banana",
             CODE == "c" ~ "cherry",
             FALSE ~ "no fruit"
           )
)



tbl <- tibble(CODE = sample(letters[1:3], 1e+06, replace = TRUE))

key <- tribble(~CODE,   ~FRUIT, 
               "a",  "apple", 
               "b", "banana", 
               "c",  "cherry"
)

tbl_with_factor <- mutate(tbl, CODE = as.factor(CODE))
key_with_factor <- mutate(key, CODE = as.factor(CODE))




```


***

```{r}
#Change "subject" element from character to a factor for analysis.
comments_rawVerYN_V$subject <- factor(comments_rawVerYN_V$subject)
str(comments_rawVerYN_V$subject)
table(comments_rawVerYN_V$subject)
```

```{r}
#Change "structure" element from character to a factor for analysis.
comments_rawVerYN_V$structure <- factor(comments_rawVerYN_V$structure)
str(comments_rawVerYN_V$structure)
table(comments_rawVerYN_V$structure)
```

```{r}
#Generate a vector of ~10,000 numbers (be sure it can be generated consistently)
#Adjust the number so it matches the number of rows in the original data frame
#Next, set the seed for random numbers, and randomize the order of the vector
vector <- 1:9340
set.seed(123) #change this seed value, and the data are randomized in a new sequence.
vector <- sample(vector)
glimpse(vector)
```

```{r}
#Append the vector as a new column to "comments_rawVerYN" with name "randomizer"
#Sort rows of dataframe according to "randomizer" column using dplyer 'arrange' function. 
comments_rawVerYN_V$randomizer <- c(vector)
comments_rawVerYN_R <-comments_rawVerYN_V %>% arrange(randomizer)
print(comments_rawVerYN_R)

```


Convert data set to a volative corpus using "tm" library.
```{r}
#Create the volatile corpus that contains the "text" vector from data frame.
comments_corpusVerYN_R <- VCorpus(VectorSource(comments_rawVerYN_R$text))
print(comments_corpusVerYN_R)

#Check out the first few text comments in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpusVerYN_R[1:3])

#Use "as.character" function to see what a single text comment looks like.
as.character(comments_corpusVerYN_R[[5]])
```

###Text Data Transformations
The text data transforms are separated out as individual "tm_map"" and "content_transformer" functions to simplify the process of testing various combinations. Conversion to lower case and removing punctuation are the baseline set. Switch between "{r}" and "{}" to turn the other transformation blocks off or on. Removing extra white spaces must be the last transformation, and is required.

```{r}
#Convert text to all lower case letters.
comments_corpus_cleanVerYN_R <- tm_map(comments_corpusVerYN_R, content_transformer(tolower))

#Remove punctuation using the "removePunctuation" function. 
#This step removes evidence of questions, so may remove data.
comments_corpus_cleanVerYN_R <- tm_map(comments_corpus_cleanVerYN_R, removePunctuation)

#Remove numerals. 
#I am not sure this is beneficial; does it remove evidence indicating technical comments?
comments_corpus_cleanVerYN_R <- tm_map(comments_corpus_cleanVerYN_R, removeNumbers)

#Stopword removal. 
#This is a standard cleanup step, but again may remove useful terms.
#The "stopwords" file is a convention; other files can be substituted for it
comments_corpus_cleanVerYN_R <- tm_map(comments_corpus_cleanVerYN_R, removeWords, stopwords())

#The option to use stemming has been removed from this version.

#Final step removes white space from the document. This is NOT an optional step.
comments_corpus_cleanVerYN_R <- tm_map(comments_corpus_cleanVerYN_R, stripWhitespace)

#Look at an example of cleaned text comment to see if it fits what is expected.
as.character((comments_corpus_cleanVerYN_R[[5]]))
```

####\  

##Basic Naive Bayes (NB) Method Using N-grams
This code block uses tm's NLP commands. The same block can be used to assess 1-, 2-, or 3-grams. Only the "#" variable in "(words(x), #)" below needs to be changed. 

```{r}
NLP_Tokenizer <- function(x) {
      unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}

comments_dtm_1gram <- DocumentTermMatrix(comments_corpus_cleanVerYN_R, control=list(tokenize = NLP_Tokenizer))
comments_dtm_1gram_tidy <- tidy(comments_dtm_1gram)
inspect(comments_dtm_1gram)
```

####\  

###Data Preparation
Split data into 75% training and 25% testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data. 

```{}
.75 * 9340 #Number of rows in the dataset; product is #rows for training
.25 * 9340 #Product is #rows for testing set
```

This code uses the comments in the order they are listed in the original file. Block for randomizing them was done earlier.
```{r}
comments_dtm_train <- comments_dtm_1gram[1:7005, ]
comments_dtm_test <- comments_dtm_1gram[7006:9340, ]
```

Save vectors labeling rows in the training and testing vectors
```{r}
comments_train_labels_subject <- comments_rawVerYN_R[1:7005, ]$subject
comments_test_labels_subject <- comments_rawVerYN_R[7006:9340,]$subject
```

```{r}
comments_train_labels_structure <- comments_rawVerYN_R[1:7005, ]$structure
comments_test_labels_structure <- comments_rawVerYN_R[7006:9340,]$structure
```

Make sure that the proportion of each sub-category is similar in the training and testing data sets.
```{r}
prop.table(table(comments_train_labels_subject))
prop.table(table(comments_test_labels_subject))
```

```{r}
prop.table(table(comments_train_labels_structure))
prop.table(table(comments_test_labels_structure))
```

####\  

###Preparation for Naive Bayes
Remove words from the matrix that appear less than 5 times.
```{r}
comments_freq_words <- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)
```

Limit Document Term Matrix to only include words in the comments_freq_words vector. Using all the rows, but we want to limit the columns to these words in the frequency vector.
```{r}
comments_dtm_freq_train <- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test <- comments_dtm_test[ , comments_freq_words]
```

The basic e1071 naive bayes classifier works with categorical features, so the matrix must be converted "yes" and "no" categorical variables. This is done using a **convert_counts function** and applying it to the data. This replaces values greater than 0 with yes, and values not greater than 0 with no. 
```{r}
convert_counts2 <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

comments_train <- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test <- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)
```

The resulting matrices have cells indicating "yes" or "no" if the word represented by the column appears in the text comment represented by the row.

####\  

###Train Model, Predict, Evaluate for Subject
This block uses the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group 1_basic, 2_writing, 3_technical, or 4_logic. It evaluates the prediction with the actual data using a crosstable from the gmodels package.
```{}
comments_classifier_subject <- naiveBayes(comments_train, comments_train_labels_subject)
comments_test_pred_subject <- predict(comments_classifier_subject, comments_test)
CrossTable(comments_test_pred_subject, comments_test_labels_subject, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

The cross table lists how many of the predicted categories are in their correct categories. Adding diagonally and dividing by total count gives the percent accuracy at predicting subject.
```{r}
print("Overall subject accuracy")
(772+1174)/23.35  #Need to find a way to calculate this automatically. 
```


###Train Model, Predict, Evaluate for structure
This is the same block as above but uses the "structure" data column.

```{r}
comments_classifier_structure <- naiveBayes(comments_train, comments_train_labels_structure)
comments_test_pred_structure <- predict(comments_classifier_structure, comments_test)
CrossTable(comments_test_pred_structure, comments_test_labels_structure, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

Again, a cross table shows how many of the predicted categories match the correct categories for structure.

```{r}
print("Overall structure accuracy")
(1117+592)/23.35  #Need to find a way to calculate this automatically. 
```

###OPTIONAL Output
To make a table showing predicted categories for comments based on the Naive Bayes test, versus the actual label entered by a human observer, merge the two datasets into a single data frame. The full data frame will be too long to display; use "head" to check format, then output the full dataset to CSV for further review. By default the CSV file should be saved to the current working directory.

```{}
predicted_category_subject <- data.frame(comments_test_pred_subject,comments_test_labels_subject)
head(predicted_category_subject)
write.csv(predicted_category_subject, file = "PredictedCategorySubject_Data.csv")
```

```{}
predicted_category_structure <- data.frame(comments_test_pred_structure,comments_test_labels_structure)
head(predicted_category_structure)
write.csv(predicted_category_structure, file = "PredictedCategoryStructure_Data.csv")
```


***
START COMMENTS, OLD work notes
***


```{}
#Creates a data frame that compares predicted and actual labels for subject and structure
tally <- data.frame("subject_label"=c(comments_test_labels_subject), "subject_predicted"=c(comments_test_pred_subject), "structure_label"=c(comments_test_labels_structure), "structure_predicted"=c(comments_test_pred_structure))
#tally <- as.numeric(as.character(tally))
```


```{}
if_else (tally(comments_test_labels_subject) != tally(comments_test_pred_subject),"YES","no")

#glimpse(tally)

#tally %>% mutate(match_subject=comments_test_labels_subject - comments_test_pred_subject)
#tally %>% mutate(match_structure=comments_test_labels_structure - comments_test_pred_structure)
```

***
END COMMENTS
***




####\  

##Initial Setup
Calling the required libraries.

```{}
library(tidyverse)
library(tidytext)
library(tidyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
```

####\  

##Version 2.0.yn of Naive Bayes (NB) Protocol
###Initial Data Input
```{}
#Read in TA comments from CSV file.
base_dataVerYN <- read_csv(file='data/coded_full_comments_dataset_Spring18anonVerYN.csv')
```

###Extract, Organize Data Subset
Extract comments and classification data subset
```{}
#Select rows representing the sub-groups to compare. 
comments_subsetVerYN <- filter(base_dataVerYN,code.subject=="1_basic"|code.subject=="2_writing"|code.subject=="3_technical"|code.subject=="4_logic")

#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
comments_rawVerYN <- comments_subsetVerYN %>% select(23,24,22)

#Rename the columns.
names(comments_rawVerYN)[1] <- "type"
names(comments_rawVerYN)[2] <- "locus"
names(comments_rawVerYN)[3] <- "text"

#Change "type" element from character to a factor for analysis.
comments_rawVerYN$type <- factor(comments_rawVerYN$type)
str(comments_rawVerYN$type)
table(comments_rawVerYN$type)
```


```{}
#Change "locus" element from character to a factor for analysis.
comments_rawVerYN$locus <- factor(comments_rawVerYN$locus)
str(comments_rawVerYN$locus)
table(comments_rawVerYN$locus)
```

***
***
Here is where I need to add the column used to randomly assign the data to training or testing sets.

Use set.seed(1234) to create a stable randomized set. 


***
***

Convert data set to a volative corpus using "tm" library.
```{}
#Create the volatile corpus that contains the "text" vector from data frame.
comments_corpusVerYN <- VCorpus(VectorSource(comments_rawVerYN$text))
print(comments_corpusVerYN)

#Check out the first few text comments in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpusVerYN[1:3])

#Use "as.character" function to see what a single text comment looks like.
as.character(comments_corpusVerYN[[5]])
```

###Text Data Transformations
The text data transforms are separated out as individual "tm_map"" and "content_transformer" functions to simplify the process of testing various combinations. Conversion to lower case and removing punctuation are the baseline set. Switch between "{r}" and "{}" to turn the other transformation blocks off or on. Removing extra white spaces must be the last transformation, and is required.

```{}
#Convert text to all lower case letters.
comments_corpus_cleanVerYN <- tm_map(comments_corpusVerYN, content_transformer(tolower))
```

```{}
#Remove punctuation using the "removePunctuation" function. 
#This step removes evidence of questions, so may remove data.
comments_corpus_cleanVerYN <- tm_map(comments_corpus_cleanVerYN, removePunctuation)
```

```{}
#Remove numerals. 
#I am not sure this is beneficial; does it remove evidence indicating technical comments?

comments_corpus_cleanVerYN <- tm_map(comments_corpus_cleanVerYN, removeNumbers)
```

```{}
#Stopword removal. 
#This is a standard cleanup step, but again may remove useful terms.
#The "stopwords" file is a convention; other files can be substituted for it

comments_corpus_cleanVerYN <- tm_map(comments_corpus_cleanVerYN, removeWords, stopwords())
```

Option to stem terms has been removed from this version.


Final prep and data check.
```{}
#Final step removes white space from the document. This is NOT an optional step.
comments_corpus_cleanVerYN <- tm_map(comments_corpus_cleanVerYN, stripWhitespace)

#Look at an example of cleaned text comment to see if it fits what is expected.
as.character((comments_corpus_cleanVerYN[[5]]))
```

####\  

##Basic Naive Bayes (NB) Method Using N-grams
This code block uses tm's NLP commands. The same block can be used to assess 1-, 2-, or 3-grams. Only the "#" variable in "(words(x), #)" below needs to be changed. 

```{}
NLP_Tokenizer <- function(x) {
      unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}

comments_dtm_1gram <- DocumentTermMatrix(comments_corpus_cleanVerYN, control=list(tokenize = NLP_Tokenizer))
comments_dtm_1gram_tidy <- tidy(comments_dtm_1gram)
inspect(comments_dtm_1gram)
```

####\  

###Data Preparation
Split data into 75% training and 25% testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data. 

```{}
.75 * 9340 #Number of rows in the dataset; product is #rows for training
.25 * 9340 #Product is #rows for testing set
```

This code uses the comments in the order they are listed in the original file. **Need a block for  randomizing them.**
```{}
comments_dtm_train <- comments_dtm_1gram[1:7005, ]
comments_dtm_test <- comments_dtm_1gram[7006:9340, ]
```

Save vectors labeling rows in the training and testing vectors
```{}
comments_train_labels_type <- comments_rawVerYN[1:7005, ]$type
comments_test_labels_type <- comments_rawVerYN[7006:9340,]$type
```

```{}
comments_train_labels_locus <- comments_rawVerYN[1:7005, ]$locus
comments_test_labels_locus <- comments_rawVerYN[7006:9340,]$locus
```


Make sure that the proportion of each sub-category is similar in the training and testing data set.
```{}
prop.table(table(comments_train_labels_type))
prop.table(table(comments_test_labels_type))
```

```{}
prop.table(table(comments_train_labels_locus))
prop.table(table(comments_test_labels_locus))
```



####\  

###Preparation for Naive Bayes
Remove words from the matrix that appear less than 5 times.
```{}
comments_freq_words <- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)
```

Limit Document Term Matrix to only include words in the comments_freq_words vector. Using all the rows, but we want to limit the columns to these words in the frequency vector.
```{}
comments_dtm_freq_train <- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test <- comments_dtm_test[ , comments_freq_words]
```


The basic e1071 naive bayes classifier works with categorical features, so the matrix must be converted "yes" and "no" categorical variables. This is done using a **convert_counts function** and applying it to the data. This replaces values greater than 0 with yes, and values not greater than 0 with no. 

```{}
convert_counts2 <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

comments_train <- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test <- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)
```

The resulting matrices have cells indicating "yes" or "no" if the word represented by the column appears in the text comment represented by the row.

####\  

###Train Model, Predict, Evaluate for Type
This block uses the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group 1_basic, 2_writing, 3_technical, or 4_logic. It evaluates the prediction with the actual data using a crosstable from the gmodels package.
```{}
comments_classifier_type <- naiveBayes(comments_train, comments_train_labels_type)
comments_test_pred_type <- predict(comments_classifier_type, comments_test)
CrossTable(comments_test_pred_type, comments_test_labels_type, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```



The cross table list how many of the predicted categories are in their correct categories. Adding diagonally and dividing by total count gives the percent accuracy at predicting TYPE.

```{}
print("Overall TYPE accuracy")
(14+45+1346+19)/23.35
```


###Train Model, Predict, Evaluate for LOCUS
This is the same block as above but uses the "Locus" data column.

```{}
comments_classifier_locus <- naiveBayes(comments_train, comments_train_labels_locus)
comments_test_pred_locus <- predict(comments_classifier_locus, comments_test)
CrossTable(comments_test_pred_locus, comments_test_labels_locus, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

Again, a cross table shows how many of the predicted categories match the correct categories for LOCUS.

```{}
print("Overall LOCUS accuracy")
(62+64+931+3)/23.35
```

###OPTIONAL Output
To make a table showing predicted categories for comments based on the Naive Bayes test, versus the actual label entered by a human observer, merge the two datasets into a single data frame. The full data frame will be too long to display; use "head" to check format, then output the full dataset to CSV for further review. By default the CSV file should be saved to the current working directory.

```{}
predicted_category_type <- data.frame(comments_test_pred_type,comments_test_labels_type)
head(predicted_category_type)
write.csv(predicted_category_type, file = "PredictedCategoryType_Data.csv")
```

```{}
predicted_category_locus <- data.frame(comments_test_pred_locus,comments_test_labels_locus)
head(predicted_category_locus)
write.csv(predicted_category_locus, file = "PredictedCategoryLocus_Data.csv")
```


***
START COMMENTS
***

##Permutation Tests of Naive Bayes (NB) Method
Using single words, Version 2.0.b correctly identifies comment "type" ~80% of the time and comment "locus" ~59% of the time.

Permutations to Test:

*  Compare 1-grams, 2-grams, 3-grams
*  Make pair-wise comparisons versus comparing multiple groups simultaneously. 
    +  Initial approach will be to split between largest group and balance in each category
    +  Ex.: type is 3-technical vs. not 3_technical

Known Issues:

1. The e1071 naive bayes classifier is binomial. Need to incorporate multinomial.


***
END COMMENTS
***



