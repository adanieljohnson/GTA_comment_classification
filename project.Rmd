---
title: "Project"
author: "Dan Johnson"
date: "10/26/2018"
output: html_document
bibliography: library.bib
biblio-style: apalike
---
```{r, child="_setup.Rmd"}
```

# Abstract 


####\

# Introduction
## Background
Undergraduate introductory STEM courses typically include technical writing training. Often it is treated as an incidental skill that each student develops "along the way." Or, the instructional strategy is  haphazardly designed, implemented, and executed. Adding to the challenge, faculty may not be directly responsible for helping students develop writing skills. 

Technical writing training usually is part of a lab course associated with a lecture class. These lab sections may be be led by graduate teaching assistants or even advanced undergraduates. These class leaders may have under-developed writing skills themselves, or lack pedagogical content knowledge needed to guide students effectively. 

We have been evaluating standardized approaches and techniques for teaching technical writing that:

*  Embody best practices from nearly 40 years of "writing in the disciplines"" literature
*  Can be implemented at large scale by less experienced teaching assistants
*  Emphasize student development through holistic coaching 
*  Focus student and instructor attention on larger issues rather than simple copy-correction.

We have amassed a structured database of laboratory reports written by undergraduate students for introductory biology courses. Besides the original texts we have collected these meta-data:

*  Circumstances: first or second report of semester; initial submission or revision; course; topic; year of student author in college
*  Instructor information: identity of TA providing feedback; score assigned; rationale behind score
*  All comments that a TA made on the individual reports they graded.

We are using text-mining approaches to map key features of student writing, as well as connections and correlations between those features, that can inform how we teach technical writing.

## Context for This Project
Students make greater gains as writers when instructors avoid just telling students what to correct ("copy editing"), and adopt a coaching- oriented approach instead. To that end, we train graduate teaching assistants use a **global first, specific later** approach to grading undergraduate technical writing. We ask them to spend a limited block of time writing fewer comments that focus on broader issues. Ideally, TAs provide no more than 3-5 comments per text page, and comments should encourage student reflection. As students' writing improves, the TA can focus on smaller issues.   

While this approach is well-grounded in research, we are uncertain how well TAs can implement the recommended strategy. One of our ongoing questions has been, "what do TAs ACTUALLY spend the most time and effort making comments about as they grade student writing?"

In Summer 2018, we began trying to answer this question by analyzing TAs' comments from student papers. We extracted and tabulated all TA comments from approximately 500 pairs of reports (initial submissions and revisions) in 3 different courses collected during a single semester. Initial data cleaning involved removing duplicates, splitting comments addressing multiple independent writing issues, and removing comments lacking any context for judging the intent. In all, the final tabulated dataset included ~11,000 separate comments.

We next used a subset of 100 randomly selected comments to build an initial qualitative codebook for sorting comments into types. The initial codebook was re-tested using 1100 additional randomly selected comments. Using the final codebook, we sorted all 11,000 comments into separate categories (see LINK for complete codebook.)

Coded TA comments were sorted on two axes: 

* __Subject__: did the comment focus mainly on Basic criteria, Technical issues, Writing quality, Logic, or Other issues
* __Structure__: was the comment mainly Copy correction, General or Specific information, Pointing to another resource, or Holistic coaching for improvement?

The resulting sorted contingency table provided a relative estimate of TA effort.

####\  

## The Central Problem
While qualitative coding of TA comments is informative, it is unsustainable. A single investigator needed 2 months to develop the codebook then rate 11,000 comments. Even with the codebook, individual coders will require training to achieve any accuracy. Given 10-12,000 comments are generate EACH SEMESTER, an automated sorting method is needed.

####\  

# Literature Review

# Methods
## Preliminary Proof of Concept

####\

## Data

####\

## Analysis

####\

# Results

####\

# Discussion

####\

# Conclusion

####\

# References

####\
