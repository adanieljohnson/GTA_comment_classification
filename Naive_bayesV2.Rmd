---
title: "Naive Bayes Workflow"
author: "Dan Johnson"
date: "12/21/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Summary
This notebook documents a basic workflow for applying a naive Bayes model to terms in TA comments. I based it on a tutorial [https://rpubs.com/Billyhansen6/318412](located here). This is quoted from Hansen's original tutorial:
>The objective of this project is to classify SMS messages as spam or ham (not spam). A Naive Bayes classifier approach will be used. This example is taken from chapter 4 of Machine Learning with R, Second Edition"

>An example of the conditional probability that will be computed is as follows: 

$$P(Spam|Hospital) = P(Hospital|Spam)P(Spam)/P(Hospital)$$ 

>which is the formula for determining the probability that a message is spam given that it contains the word "Hospital" in the message.

Lines ~30-205 are the baseline version of the workflow using single words. It correctly classifies comments in the top 4 sub-categories  ~79% of the time. While that is close to minimum acceptable inter-rater reliability of two human readers, it is well below my goal of >95% for use in routine automated analysis. 

Lines ~208-290 describe an alternative method using n-grams. When I tested this approach using 2-grams, classification was slightly worse, at 68.3% correct. 

Lines ~295-335 show how to use word clouds to visualize single word frequencies. I did not find this approach very useful.

To keep this code clean for now I will run various permutations in a scratch version, then summarize findings starting on line #345. In the subsequent iterations I will evaluate:

*  Different text pre-processing strategies: stemmed versus unstemmed, numbers removed vs. not, etc.
*  Using n-grams rather than single words
*  Making pair-wise comparison versus comparing multiple groups simultaneously
*  Modifying the variables in the NB analysis itself.

Finally, there are several things still missing or needed for this workflow:

*  How to feed a text dataset in, and get a table of the predicted categories back out.
*  How to store the values from the Bayes prediction table so I can use them in calculations rather than type them in by hand.
*  How to randomize which comments are used for training, testing datasets.

Other pages examine other classifiers besides Naive Bayes.

####\  

##Initial Setup
```{r}
library(tidyverse)
library(tidytext)
library(tidyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
```

####\  

##Baseline Version 1.0 of Naive Bayes (NB) Protocol
###Initial Data Input
```{r}
#Read in TA comments from CSV file.
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')

#Select rows representing the sub-groups to compare. 
comments_subset <- filter(base_data,code.subject=="1. Basic Criteria"|code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")

#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
comments_raw <- comments_subset %>% select(23,22)

#Rename the columns.
names(comments_raw)[1] <- "type"
names(comments_raw)[2] <- "text"

#Simplify coding terms
comments_raw[,1] <- ifelse(comments_raw[,1] == "1. Basic Criteria","1_Basic", ifelse(comments_raw[,1] == "2. Writing Quality","2_Writing", ifelse(comments_raw[,1] == "3. Technical and Scientific","3_Technical", ifelse(comments_raw[,1] == "4. Logic and Thinking","4_Logic",99))))

#Change "type" element from character to a factor for analysis.
comments_raw$type <- factor(comments_raw$type)
str(comments_raw$type)
table(comments_raw$type)
```





##PROBLEM
The set.seed command can pull random lines from a data table, but does not work on a document term matrix. 

```{r}
set.seed(1234) 		# ensures the same random set gets picked each time.

comments_raw_dtm_train <- sample_frac(comments_raw, .75)	# sample_frac pulls out 75% of "comments_raw" dataset randomly using the sample sequence identified by set.seed(1234)

comments_raw_dtm_test <- setdiff(comments_raw, comments_raw_dtm_train)	#setdiff creates a "test" dataset using anything NOT in the "comments_raw_dtm_train" subset of "comments_raw."
```

Data set must be converted to a volative corpus using "tm" library then transformed.
```{r}
#Create the volatile coprus that contains the "text" vector from data frame.
comments_corpus_train <- VCorpus(VectorSource(comments_raw_dtm_train$text))
print(comments_corpus_train)

#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpus_train[1:3])

#Use "as.character" function to see what a message looks like.
as.character(comments_corpus_train[[3]])
```


```{r}
#Create the volatile coprus that contains the "text" vector from data frame.
comments_corpus_test <- VCorpus(VectorSource(comments_raw_dtm_test$text))
print(comments_corpus_test)

#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpus_test[1:3])

#Use "as.character" function to see what a message looks like.
as.character(comments_corpus_test[[3]])
```

***
***
The OPTIONAL data transforms start here. They use "tm_map"" and "content_transformer" functions. Switch between "{r}" and "{}" to turn a transformation block off or on.
```{r}
#Convert to all lower case letters.
#Pretty standard conversion.
comments_corpus_clean_train <- tm_map(comments_corpus_train, content_transformer(tolower))
comments_corpus_clean_test <- tm_map(comments_corpus_test, content_transformer(tolower))
```

```{r}
#Remove punctuation using the "removePunctuation" function. 
#This step removes evidence of questions, so may remove data.
comments_corpus_clean_train <- tm_map(comments_corpus_clean_train, removePunctuation)
comments_corpus_clean_test <- tm_map(comments_corpus_clean_test, removePunctuation)
```



```{}
#Remove numerals. 
#I am not sure this is beneficial; does it remove evidence indicating technical comments?
comments_corpus_clean <- tm_map(comments_corpus_clean, removeNumbers)
```

```{}
#Stopword removal. 
#Can be refined by altering what reference file is used.
comments_corpus_clean <- tm_map(comments_corpus_clean, removeWords, stopwords())
```

```{}
#Stemming the text data to strip the suffix from words. 
#My thinking is that stemming removes valuable tense data.I am unsure how valuable it is.
#Alternative is using part-of-speech tagging and lemmatization instead.

comments_corpus_clean <- tm_map(comments_corpus_clean, stemDocument)
```

Final prep and data check for training set.
```{r}
#Final step removes white space from the document. This is NOT an optional step.
comments_corpus_clean_train <- tm_map(comments_corpus_clean_train, stripWhitespace)

#Look at an example of data.
as.character((comments_corpus_clean_train[[3]]))
```


Final prep and data check for testing set.
```{r}
#Final step removes white space from the document. This is NOT an optional step.
comments_corpus_clean_test <- tm_map(comments_corpus_clean_test, stripWhitespace)

#Look at an example of data.
as.character((comments_corpus_clean_test[[3]]))
```
####\  

##Modified Naive Bayes (NB) Method Using N-grams
The tokenizer command block below uses tm's NLP commands to generate either 1-, 2-, or 3-grams with the same command. The only change needed is "#" in "(words(x), #)" below.

```{r}
NLP_Tokenizer <- function(x) {
      unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}

comments_dtm_ngram_train <- DocumentTermMatrix(comments_corpus_clean_train, control=list(tokenize = NLP_Tokenizer))
comments_dtm_ngram_train_tidy <- tidy(comments_dtm_ngram_train)
inspect(comments_dtm_ngram_train)
```


```{r}
NLP_Tokenizer <- function(x) {
      unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}

comments_dtm_ngram_test <- DocumentTermMatrix(comments_corpus_clean_test, control=list(tokenize = NLP_Tokenizer))
comments_dtm_ngram_test_tidy <- tidy(comments_dtm_ngram_test)
inspect(comments_dtm_ngram_test)
```

####\  

###Data Preparation

```{r}
#comments_dtm_train <- comments_dtm_ngram_train
#comments_dtm_test <- comments_dtm_ngram_test
print("Train")
glimpse(comments_dtm_ngram_train)
print("Test")
glimpse(comments_dtm_ngram_test)
```

Save vectors labeling rows in the training and testing vectors
```{r}
comments_train_labels <- comments_raw_dtm_train$type
comments_test_labels <- comments_raw_dtm_test$type
print("Train")
glimpse(comments_train_labels)
print("Test")
glimpse(comments_test_labels)
```

Make sure that the proportion of each sub-category is similar in the training and testing data set.
```{r}
prop.table(table(comments_train_labels))
prop.table(table(comments_test_labels))
```

####\  

###Preparation for Naive Bayes
Remove words from the training matrix that appear less than 5 times.
```{r}
comments_freq_words_train <- findFreqTerms(comments_dtm_ngram_train, 5)
str(comments_freq_words_train)
```

Remove words from the test matrix that appear less than 5 times.
```{r}
comments_freq_words_test <- findFreqTerms(comments_dtm_ngram_test, 5)
str(comments_freq_words_test)
```


Limit our Document Term Matrix to only include words in the sms_freq_vector. We want all the rows, but we want to limit the column to these words in the frequency vector.
```{r}
comments_dtm_freq_train <- comments_dtm_ngram_train[ , comments_freq_words_train]
comments_dtm_freq_test <- comments_dtm_ngram_test[ , comments_freq_words_test]
```

The naive bayes classifier works with categorical features, so we need to convert the matrix to "yes" and "no" categorical variables. To do this we'll build a convert_counts function and apply it to our data.
```{r}
convert_counts2 <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
```

This replaces values greater than 0 with yes, and values not greater than 0 with no. Let's apply it to our data.
```{r}
comments_train <- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test <- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)
print("Train")
glimpse(comments_train)
print("Test")
glimpse(comments_test)
```

The resulting matrixes will be character type, with cells indicating "yes" or "no" if the word represented by the column appears in the message represented by the row.

####\  

###Train Model, Predict, Evaluate
Use the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group ONE, TWO, THREE, or FOUR. Evaluate the prediction with the actual data using a crosstable from the gmodels package.

***
Here is where it breaks. It generates the comparison data (Step 1) and I can glimpse all the inputs and first output (see prior blocks). I just cannot run the prediction builder step. I keep getting:

```
Error in `[[<-.data.frame`(`*tmp*`, i, value = integer(0)) : 
  replacement has 0 rows, data has 1946
```


```{r}
comments_classifier <- naiveBayes(comments_train, comments_train_labels)

comments_test_pred <- predict(comments_classifier, comments_test)

CrossTable(comments_test_pred, comments_train_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```


***
***
####\  

##Visualization
This code generates a wordcloud of the frequency of single words in the dataset using the package "wordcloud".
```{r}
wordcloud(comments_corpus_clean, max.words = 50, random.order = FALSE)
```

Compare wordclouds between 3 groups.
```{r}
ONE <- subset(comments_raw, type == "1_Basic")
TWO <- subset(comments_raw, type == "2_Writing")
THREE <- subset(comments_raw, type == "3_Technical")
FOUR <- subset(comments_raw, type == "4_Logic")

wordcloud(ONE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
```

Show the 5 most frequent words in the data:
```{r}
comment_sack <- TermDocumentMatrix(comments_corpus_clean)
comment_pack <- as.matrix(comment_sack)
comment_snack <- sort(rowSums(comment_pack), decreasing = TRUE)
comment_hack <- data.frame(word = names(comment_snack), freq=comment_snack)
head(comment_hack, 10)
```

And the 5 most frequent words from each class:
```{r}
wordcloud(ONE$text, max.words = 10, scale = c(3, 0.5))
wordcloud(TWO$text, max.words = 10, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 10, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 10, scale = c(3, 0.5))
```

####\  

##Permutation Tests of Naive Bayes (NB) Method
Version 1.0 correctly identifies comment subcategories ~79% of the time when using single words, and 68.3% of the time when using 2-grams. Permutations to test:

*  Compare 2-grams, 3-grams rather than single words
*  Make pair-wise comparisons versus comparing multiple groups simultaneously. 
    +  Ex.: Basic vs. Not Basic (1_Basic vs. not 1_Basic)
*  Modify the variables within the NB analysis itself. 
    +  Laplace constant (Y/N), Cutoff value for words removed from list.
*  Do/Do not stem the terms
    +  This may remove evidence of tense
*  Remove/Leave stopwords
*  Remove/Leave capitalization
*  Remove/Leave numbers
    +  This may remove evidence of technical descriptors
*  Remove/Leave punctuation
    +  This may remove evidence of questions vs statements


