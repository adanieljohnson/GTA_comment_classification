This project team’s long-range goal is to improve scientific writing skills of undergraduate students in a wide range of colleges and universities, by promoting widespread adoption of evidence-based writing training. An important step towards this long-term goal is reducing the practical barriers that keep faculty from routinely including scientific writing in post-secondary STEM classes; that is the focus of this particular project. 
The central research question this project asks is whether an integrated writing training program that combines text reading and annotation exercises, regular automated feedback on writing, and holistic feedback from instructors accelerates development of students’ scientific writing skills. Does it increase student confidence in their writing abilities? Does it improve instructors’ grading consistency and reduce time spent grading? Can it make assessment more transparent for teachers and students? Does it encourage adoption of evidence-based scientific writing instructional practices? 
The writing training program being evaluated has three linked elements. The first element is the STEM Automated Writing Help Tool (SAWHET) created by the investigators on this proposal. It is an automated system for collecting lab reports, conducting text analysis, then giving students tailored feedback. Reports go on to instructors for additional comments, and annotated copies are stored in SAWHET’s archive. Automated feedback provides students with actionable data about their drafts and gives instructors more data for evaluating a student’s writing. SAWHET has been used to collect lab reports electronically since Fall 2015 and already contains a research dataset of over 1500 student lab reports.
The second element is BioTAP (Biology Thesis Assessment Protocol). This is a previously NSF-funded writing training and evaluation protocol created to assess undergraduate research theses across multiple disciplines. Unlike most science writing training models BioTAP takes a holistic view of writing evaluation and feedback that reflects best practice principles from Writing in the Disciplines and Writing to Learn paradigms. 
The third element of the training model is directed student reading and text annotation exercises. These introduce students to the structure of scientific papers and important universal features of scientific writing before students start writing on their own. This “reading first” approach accelerates students’ adoption of these features into their own writing. Students will annotate published articles and training documents using the same BioTAP rubric and scoring criteria that instructors will use to score the students’ written work later in the semester. 
Brought together, BioTAP provides a uniform method for teaching writing and standards for holistic, process-oriented assessment. Reading annotation focuses instructor and student attention on the appropriate elements of writing before students actually begin. SAWHET creates additional opportunities for students to get feedback and revise their work multiple times, and provides consistent format. 
Funding from NSF will support three interconnected lines of work. The project team will test impacts of an integrated scientific writing training program with these three elements on a study population of ~400 students and ~20 graduate teaching assistants over the course of 3 years. NSF funding also will support coding and implementation of additional natural language processing algorithms into SAWHET that can identify all of the key features listed in the BioTAP assessment rubric. By the end of the grant period SAWHET will be able to provide effective, actionable feedback on student writing that can be incorporated into large classes. NSF funding also will support computational linguistic analyses of student writing in STEM to find patterns that are statistically correlated to quality of written work. 
Intellectual Merit: this project is a controlled comparison of student learning outcomes with vs. without technology support as part of a well-defined instructional intervention. It will contribute to understanding how technology can support development of scientific writing skills. Where prior studies used self-reported benefits, this study will look directly at technology impacts by measuring much automated feedback students actually use, and how that translates into progress towards writing mastery. The SAWHET platform and algorithms can open up new areas for STEM education research by simplifying and standardizing collection, processing, and most important, analysis of student writing. The project corpus of ~ 4800, annotated undergraduate lab reports can be used as training data for linguistic analyses, to test other automated systems, and for other research.
Broader Impacts: this project will improve post-secondary biology education by fostering adoption of evidence-based scientific writing training practices. The training and assessment model follows principles of practice from Writing in the Disciplines and Write to Learn (WID/WTL) curricula that >50%f of colleges and universities use for general education writing programs. Adoptees’ in-class efforts are connected automatically to the larger WID/WTL community in their own institutions, and nationally. With minimal changes the same model can benefit new faculty, AP Biology teachers, and instructors in advanced college courses. The project also helps address a need for GTA teaching professional development. Currently, a majority of GTAs participate in a one-time workshop as their only form of training. Many institutions would like to provide more but are unsure how to structure such programs. Products of this project include a fully developed 1-year GTA training and teaching professional development program.

Project Description
This project aims to reduce both pedagogical and practical implementation barriers to scientific writing training in biology courses using a combination of a new technology-assisted, automated feedback system (SAWHET) with novel and previously validated writing training exercises. Its central research question is whether an integrated writing training program of text reading and annotation, routine automated feedback, and individualized context- specific coaching from instructors:
●	accelerates student progression to mastery of scientific writing skills;
●	improves student confidence in their scientific writing skills;
●	improves instructors’ grading consistency while reducing time spent grading;
●	makes the assessment process more transparent; and
●	encourages adoption of evidence-based scientific writing practices.
 

This project aligns with multiple IUSE goals. Identifying new ways to use technological support to improve undergraduate scientific writing training helps “accelerate improvements in the quality and effectiveness of the education of undergraduates in all STEM fields.” Accelerating students’ progression to writing mastery promotes communication and critical thinking skills, which in turn “improve[s] preparation of students so they can succeed as productive members of the future STEM workforce.” Strong communication and critical thinking skills also help more undergraduates become “members of a STEM-literate public ready to support and benefit from the progress of science.” (Quotes are from IUSE program solicitation NSF 15-585).
The project also aligns with priorities of the joint NSF/Dept. Education Common Guidelines for Education Research and Development (NSF 2013). Specifically, this project will:
●	“Use research evidence and novel strategies to address STEM education challenges.”
●	“Draw on the existing knowledge base…and proven practices...in STEM undergraduate education.”
●	“Use and build evidence about improved STEM instructional practices.”
●	“Create, implement, and test technology-driven models.”
●	“Develop and validate assessments for undergraduate STEM learning and instructional practice.”
●	“Improve the knowledge base for... effective undergraduate STEM education teaching and learning.”
●	“Foster...use of evidence-based resources and pedagogies in undergraduate STEM education.”

This project also addresses 4 of 7 core strategies for improving postsecondary STEM training identified by the Sourcebook for Advancing and Funding Undergraduate STEM Education (Coalition for Reform of Undergraduate STEM Education, 2014):
●	Supporting broader implementation of evidence-based practices
●	Supporting continued examination and adoption of successful curricular approaches
●	Expecting institutions to address systemic change through measurable indicators and evaluation
●	Strengthening teacher preparation

 
A. Background
Strong scientific thinking & communication skills are essential for 21st century STEM workers (Anderman, 2011; Council of Writing Program Administrators 2011; President’s Council of Advisors on Science and Technology, 2012), but there are not enough college graduates with these skills to meet projected US workforce demands (Association of American Colleges & Universities, 2007; Business-Higher Education Forum, 2011; Carnevale et al., 2011.) Policy makers have called on educators to prioritize development of career-relevant “21st-century skills” (Council of Writing Program Administrators 2011; National Research Council 2010, 2012; Starke-Meyerring et al., 2011). These include quantitative and information literacy, adaptability, novel problem-solving, self-regulation, systems thinking, analytical and critical thinking skills, and complex communication skills (Anderman 2011) A more scientifically and computationally literate citizenry also will be better prepared to evaluate personal choices, societal issues, and larger policies that have deep connections to science and technology (President’s Council of Advisors on Science and Technology, 2012.) 
While all of the aforementioned 21st-century skills are vital for college graduates, we have focused specifically on improving written communication in the form of scientific writing, for several reasons. First, these skills do not develop in isolation. As students master complex communication, they also develop critical thinking skills (Anderman 2011; Carpenter 2001; Gorzelski et al., 2016; Rounsaville et al., 2008; Reiff et al., 2011; Rogers et al., 2011; Starke-Meyerring et al., 2011; White et al., 2009). By improving written communication we also can impact other skills. Second, good communication skills cut across all STEM fields. Even individuals working in relative isolation routinely record data and observations, interpret evidence, and share findings informally or formally either orally or in writing. Scientific writing skills are also valued and highly sought by employers independent of discipline or field of study (Treadwell et al., 1999; Gray et al., 2005; Fry, 2014). Third, writing is not just a mean for communication. It is also a tool to teach and develop scientific thinking. Scientific writing proficiency helps students be more literate and focused, and move from intuition-based thinking towards evidence-based thinking (Libarkin, et al. 2012; Quitadamo et al., 2007; Fellows, 1994 and references therein). 
Given projected workforce needs and that strong communications skills are vital across the range of careers, teaching scientific writing should be an obvious priority in STEM courses. Ideally, instructors would give students regular opportunities to practice critical and applied thinking by assigning writing routinely (Reynolds et al. 2012; Holyoak 1998; Anderman 2011), but there are significant logistical barriers to this pedagogical ideal (Haswell 2006a, b; Henderson, et al., 2007). 
The most obvious logistical barrier is workload. Good scientific writing is one of the most difficult and time-consuming skills to teach. It is an iterative cycle of writing drafts, receiving actionable feedback and coaching, then making further refinements until a final product emerges. This cycle is too time- and labor-intensive to be practical in high-enrollment introductory courses. Even in lower-enrollment courses with more than 10-20 students, multiple assignments pose significant logistical challenges (Coil et al. 2010; Fry 2014). When students do have an opportunity to write for class, circumstances may permit just one round of revisions. The feedback they receive needs to be as focused, individualized, and impactful as possible. 
Giving effective feedback is a challenging and complex task that needs to be learned through specific training (Bazerman 1994; Bazerman, et al. 2006; Gottschalk 2003; Graves 2011; Kiefer et al., 2002; National Council of Teachers of English/National Writing Program; Perelman 2009; Szymanski 2014; and Underwood et al., 2006 for reviews). Effective feedback will be process-oriented rather than focused on finding and correcting errors. Comments will be understandable and unambiguous. Effective feedback keeps the writer in control and encourages them to revise their draft thoughtfully and thoroughly, and keeps copy-editing feedback to a minimum (Breidenbach, 2006; Ruegg 2015; see Trupiano 2006 for a review of strategies promoting meaningful revision). Over-commenting or giving overly prescriptive comments can push a student writer into a passive role. They treat comments as a to-do list that needs to be completed rather than suggestions for reflection and self-assessment (Anson 2001; Council of Writing Program Administrators, 2011; Harris, 1979; Haswell 2006a,b; Reynolds et al. 2009; Szymanski 2015; Underwood et al., 2006; Swilky 1991). 
There is clear need for more robust student training and assessment models that make it easier to give students multiple cycles of “holistic feedback,” that is, feedback that helps undergraduates identify their current shortcomings as a writer and directs them towards a path to improvement. An ideal model would: 1) embody best practices for teaching writing; 2) reduce instructor workload; 3) stress holistic feedback rather than copy-editing; 4) limit the overall number of comments to what students can manage; and 5) help students understand feedback and prioritize revision efforts appropriately. This project focuses specifically on improving the feedback process in two ways. First, the training program teaches instructors how to give holistic feedback that is individualized and context-specific. Second, students will receive a portion of their feedback from SAWHET, a technology-assisted, automated feedback system. 


A.1. There is a Growing Need and Potential Role for Automated Feedback on Writing Process
Interest in technology-assisted automated feedback on writing began in the mid-1960s; working systems first appeared in the 1970s and early 1980s (Page 1968; Rothermel 2006; Slotnick 1971; Brock 1995; Haswell 2005, 2006a,b; Brent et al. 2006.) Today, spelling and grammar checkers that provide feedback on the mechanical elements of writing are mature technologies that are part of nearly all web-based and desktop text programs. Automated essay scoring (AES) systems that provide feedback on the content of open-response writing such as essays or pre-written exam prompts have gained popularity in recent years. They have the advantage of being scalable and are very attractive for large face-to-face courses, and for asynchronous courses like massive open online courses (Balfour 2013). Demand for AES systems has spurred development of commercial products including Criterion®, WriteToLearn™, Intelligent Essay Assessor™, IntelliMetric®, WriteLab, Inc. and Turnitin Feedback Studio. AES systems use natural language processing and machine learning algorithms to produce a score that is based on syntactic and lexical patterns and features such as sentence length, word frequency and part of speech (Shermis et al. 2010; Balfour 2013).
AES systems are not without drawbacks. First, it is controversial how accurately unsupervised AES systems score student work. Perelman (2013) highlighted the poor accuracy of extant AES by using obviously faked texts to demonstrate how easily current AES applications are fooled. Second, it is unclear to what extent students pay attention to automated feedback. One small study by Heffernan et al. (2015) compared relative impacts of teacher feedback versus technology driven feedback from ETS' Criterion® tool on 12 Japanese English-foreign- language students' writing. Results showed automated feedback alone was less effective than feedback from the teacher and Criterion® combined. However, the authors only compared scores on three independent, consecutive writing assignments; they did not compare effects of feedback on draft versus final versions of the same assignment. Also, the authors failed to compare automated feedback alone to teachers' comments alone. The degree of authority that students attach to automated feedback may also be a confounding variable. Ruegg (2015) found that feedback from peer reviewers was not valued as much as teachers' feedback, even though teachers provided less explicit comments than peer reviewers. Ruegg argued that this is likely because teachers are the ones that ultimately grade the student's writing. More research is needed to improve AES systems and to test their effectiveness in the classroom.
Despite their limitations, demand remains high for AES systems, and there are ongoing efforts to integrate them into more traditional writing review models (Balfour 2013). Constructed response assessments reveal more about student thinking and the persistence of misconceptions than do multiple-choice questions. However they require more analysis on the part of the educator, and so are less likely to be used in large-enrollment courses. Urban-Lurain and colleagues developed automated assessment of constructed response (AACR) as a way to give students constructed response test questions and other work that can be graded automatically (Kaplan, et al. 2014; Weston, et al., 2015; Ha, et al. 2015; Urban-Lurain, et al. 2015; Ha, et al. 2016.) AACR uses a robust and scalable platform for scoring open responses to pre-written prompts that probe important disciplinary concepts from biology and biological chemistry. The prompts are administered via online course management systems where students enter responses. Lexical and statistical analyses then predict what expert ratings would be for student responses. Work to date shows that automated analyses can predict expert ratings of students' work on these topics accurately and with higher inter-rater reliability than a group of trained human graders. The AACR model is growing rapidly to include additonal STEM disicplines and has a national presence (http://create4stem.msu.edu/project/aacr). 
The success of AACR would suggest that the general lexical analysis principles this proposal is based on can be applied to the analysis of student writing. Also, while the automated system described later in this proposal and AACR do share some methods, we see them as complementary projects, not competitors. AACR uses pre-defined prompts and focuses specifically on assessing student knowledge and higher thinking. Our proposed technology system focuses specifically on assessment of free-form writing in the context of lab reports, and on the development of undergraduate technical writing skills. Our larger program has a second distinct goal of supporting graduate teaching assistants’ professional development. 
In this proposal we are asking two questions about automated feedback that have not been addressed previously. First, does automated feedback within a larger writing training program improve students’ skills progression towards mastery? Proposed activities will test whether students who act on automated feedback recommendations: 1) show greater progress towards scientific writing skills mastery, and 2) have greater self-efficacy for scientific writing, than students who do not. Our second question is, will automated feedback affect instructors’ grading behavior for writing-related activities? Specifically, we will test the hypothesis that graduate teaching assistants (GTAs) who use a training model that includes automated feedback support: 1) spend less time grading overall; 2) provide students with more feedback focused on higher order thinking; and 3) grade more consistently, with higher inter-rater reliability.


A.2. Future Faculty Need Training on How to Teach Scientific Writing
A major barrier to routine writing in STEM is limited instructor awareness of evidence-based training models. Many college STEM teachers have not been introduced to proven writing training strategies or trained to give effective, context-specific feedback (Coil et al. 2010). Policymakers are calling for faculty development programs that specifically address STEM education priorities (National Research Council, 2010, 2012; Starke- Meyerring et al., 2011) including development of faculty across career stages and preparation of doctoral students and postdocs to be effective teachers. This needs to include how to teach science writing.
Knowing which instructional models are effective is not sufficient to ensure adoption. Faculty may have different priorities or hindering attitudes by the time they teach their first classes as assistant professors. These attitudes underlie comments like “I don’t have time for something new,” “I don’t have spare class time to spend teaching writing,” or “students should learn how to write elsewhere” (Coil, 2010; Clughe et al., 2012; FSSE, 2016). These attitudes are very resistant to change. For example, despite a nationwide multi-year effort promoting interactive, inquiry- and skills-centric teaching, many faculty still believe their main goal should be content delivery, not development of students’ disciplinary thinking and process skills (D’Avanzo 2013; Freeman et al., 2014; Gottschalk 2003; National Research Council 2012; Vision and Change 2011; FSSE 2016).
Even when faculty are motivated to innovate, how they learn about alternative strategies affects the odds of adoption. For example, faculty who attend formal professional development activities that promote evidence- based practices are less likely to adopt those practices than if they learn about them from trusted colleagues or respected opinion leaders (Andrews 2016). In contrast, graduate students who participate in formal professional development about learner-centered instruction readily adopt learner-centered methods into their personal teaching practice (Ebert-May et al., 2015). A more effective strategy for changing faculty instructional practices may be to target and train future faculty (Tanner 2006 and references therein) who then act as change agents.
A central goal of this project is to test a scalable model for teaching GTAs about the importance of scientific writing in students’ intellectual development and training them to use best-practice models. Specifically, TAs will learn how to: 1) lead students through the instructional process, 2) use validated methods and rubrics, 3) provide effective, growth-oriented feedback, and 4) grade consistently. One of the questions to be tested in this project is whether GTA conceptual knowledge and self-efficacy related to teaching scientific writing training will increase over the course of the program described in Project Activities.

B. Prior Work
B.1. Building a System of Automated Text Collection, Analysis and Curation
Two years ago (with support from the Arthur Vining Davis Foundation), the PI (Johnson) and co-PI (Setaro) began developing SAWHET (STEM Automated Writing HElp Tool). It is an automated system for collecting lab reports, conducting in-depth text analysis, then giving instructors and students feedback. In its current form, students submit draft and final reports from lab coursework into SAWHET via secure web form (Figure 2). Each section of a report (title, abstract, introduction, etc.) is entered as separate blocks. The form has on-demand help functions so students can review requirements for each section. Form filters ensure each block entry meets certain requirements prior to submission (e.g. word count). These barriers minimize submission of incomplete reports, helping students be organized, and saving GTAs from reading incomplete reports.
Submitted reports are processed and analyzed by our local GATE (General Architecture for Text Engineering, http://gate.ac.uk) text processing environment. GATE-processed texts are further compared to reports already in the SAWHET archive using R (The R Project for Statistical Computing). Finally, GATE forwards the report and analyses to the appropriate GTA for grading and commenting. 
GATE was selected for the base toolset because it has built-in validated analysis workflows that expand easily, and because it supports both rules-based analysis and natural language processing (Cunningham et al. 2013.) GATE also can use machine learning algorithms from the Natural Language Toolkit (Loper et al., 2002) and Weka (Hall et al. 2009) libraries. GATE provides GTAs with additional information for evaluating a student’s writing. For example, our GATE system already can detect whether a hypothesis is present in the Introduction section of a student report with an overall accuracy of 0.91 and a precision of 0.78 for our testing dataset. It also can calculate the readability score of each section and detect whether citations are present. The report including GTA’s comments and final grade are returned to the student as Word documents. Lab reports are stored in the SAWHET archive.
SAWHET currently only sends feedback in the form of analyses (number of citations, total words used, etc., see Fig. 2) to GTAs, not to undergraduates. With support from NSF for this project, we will develop new feedback algorithms that match our project’s writing assessment model. Outputs of these new algorithms will become part of the automated feedback that BOTH students and GTAs receive. 
 

B.2. Historical Data Collection
Using SAWHET we began collecting lab reports electronically in Fall 2015 in 3, 100- and 200-level introductory biology courses (Evolution and Ecology, Cell Biology and Comparative Physiology.) To date, we have archived ~1450 lab reports, including ~300 draft/final report pairs that will be used as part of the historical control group. The SAWHET archive of structured documents and analyses will be a primary data source for this project. We also have historical data about student’s attitudes and self-efficacy towards writing-related tasks from Biology Assessment Exams that are routine assessments at Wake Forest University (WFU) since 2014. The exam is administered each August to first year students starting their first biology courses, then again in April to students completing the last of the 4-course core curriculum. Students complete lab evaluations online each semester during the last regular lab meeting. Questions are based on the Colorado Learning Attitudes about Science Survey For Biology (Semsar, 2011) and the Biology Self-Efficacy Scale (Baldwin, 1999).

B.3. Assembly of a Test Corpus of Student Writing
For pilot studies, the PI (Johnson) and Co-PI (Setaro) assembled a test corpus of 40 undergraduate lab reports from the SAWHET archive. A sample of 10 representative papers each was selected for 4 grade bins (A, B, C, D), based on initial grades assigned by students’ GTAs. Reports originally had been graded using an internally developed rubric, not the BioTAP rubric, by a GTA with 2+ semesters of teaching experience at WFU. Initial grades were separately re-checked by the PI and Co-PI; only reports for which the original GTA score, PI score, and Co-PI score matched were retained in the test corpus. 

B.4. Lexical Complexity as a Writing Quality Metric
For this preliminary study, the PI and Co-PI compiled five wordlists representing different levels of lexical sophistication. The wordlists are mutually exclusive, except for ~100 terms with both general and discipline- specific meanings. These overlapping terms were excluded from the analysis.
1.	Stop/Filler Words (602 terms; from Glasgow and MySQL stopwords lists)
2.	Essential and Basic English Terms (10,666 terms; from New General Services, Ogden’s minimal Basic English Words, Wikipedia Basic English Words, AECMA, and America Special English Lists)
3.	General Written English Terms (52,732 terms; subset of the 100,000 most used English words from Corpus of Contemporary American English)
4.	Academic English Terms (29,455 terms; from New Academic Words List and academic subset of Corpus of Contemporary American English)
5.	Technical and Discipline Specific Terms (6,545 terms and phrases unique to life sciences disciplines. Assembled from a corpus containing 7 online texts written for general biology at the introductory undergraduate or advanced high school level, and all articles in Wikipedia containing “biology” as a key term. Cross-referenced to high-usage terms in science and technology sub-genre of the Corpus of Contemporary American English)
Separate wordlists and frequency counts were generated for each of the 40 individually validated student reports in the test corpus using GATE, then the words parsed into one of the five categories above. Overall, less than 5% of all terms used in student reports in the test corpus could not be mapped to one of the lists; most were personal names of students, GTAs or instructors, institutional locations, idiomatic scientific terms, or slang.
We found several important differences in word usage patterns and complexity between reports from different grade bins.
•	Vocabulary range declined along with overall grade. Average number of unique words used declined ~10% from “A” to “D” reports. Numbers of unique academic terms used dropped 19%, and unique technical terms used dropped 43%. This suggests students with lower grades are not just struggling with the disciplinary language, but using a narrower vocabulary overall.
•	“A” and “B” reports contained ~ 20% more general academic terms than “C” and “D” papers. Both “A” and “B” papers contained the same total number of technical terms, but the total number of technical terms used declined ~11% in C papers and ~26% in D papers. These results suggest lower-performing students are less able to use the technical and academic language they do possess.

These initial findings play into experiences many experienced instructors have, when they say that they “just know poor writing when they see it.” We will test how these results correspond to the BioTAP rubric and can be used to give effective automated feedback (see C.7. Data Analyses). One of the hypotheses we will test is that specific subsets of the 5 terms categories are more or less frequent in high vs. low performing students. 

B.5. Prior Work of Others Related to the Project
The project team’s integrated model for training students and GTA instructors uses BioTAP, an NSF-funded evaluation protocol for undergraduate theses created by Dr. Julie Reynolds (Dept. Biology, Duke University), who is an external consultant for the project and is providing her guidelines and rubrics for GTA and student training. The original BioTAP was designed to be a dual formative and summative assessment tool for undergraduate theses in biology. Although developed originally to meet specific needs of one undergraduate biology department, the model proved useful in other STEM departments (Dowd, 2014). The BioTAP protocol reflects several best practice principles from the Writing Across the Curriculum/ Disciplines and Writing to Learn models (Anson, et al. 2003; Artemeva, et al. 2003; Bahls, 2012; Bazerman, et al. 2006; Carpenter,. 2001; Moskovitz, et al., 2005; Patton, 2011; Perelman, 2009.) 
BioTAP’s main tool is a holistic rubric that defines specific categories on which writing will be assessed, and criteria and expectations for each category. Holistic rubrics like these can help undergraduate writers learn and understand the conventions and expectations of a genre of writing they have not experienced previously. Holistic rubrics make it easier to identify and explain the strengths and weaknesses of drafts and provide meaningful, actionable feedback for improvement (Moskal 2000; Durst 2006; Reynolds et al., 2009).
The original rubric has four sections. Part 1: Higher Order Writing Issues (5 questions), focuses on whether the work targets the intended audience, places the work in the context of scientific literature, and explicates the research aims. Part 2: General writing Issues (4 questions), focuses on overall organization, spelling, grammar, punctuation, citations, and presentation of figures and tables. Part 3: Quality of Scientific Work (4 questions), focuses on the experimental design and execution of the research. Guide questions in each section reflect the detailed explanations of expectations and descriptions of different performance standards. Part 4 contains criteria for receiving credit for the completed thesis, and will not be used for this project.
The BioTAP assessment rubric focuses mainly on universal elements of high-quality scientific writing that can be evaluated independently of subject matter: lexical and grammatical skills, organization and flow, critical thinking skills, quality of research, and disciplinary knowledge. BioTAP was validated previously on a cohort of 47 students and showed positive effects in students' ability of writing and high-order thinking skills, as well as increased instructor efficiency in evaluating students' theses (Reynolds et al. 2009, 2012). 
The editorial workflow for the BioTAP protocol is modeled after the process of publishing academic research, and thus provides students with an authentic learning experience. One feature of BioTAP is that students write summary letters for each draft they submit for review that identify specific issues that they most want feedback on, or specific changes they made. This helps students take responsibility for writing decisions, and mirrors the revise-and-resubmit process for most scientific journals. 
Duke University where BioTAP was developed and WFU are similarly sized institutions with similar student populations. In the current proposal a modified BioTAP rubric (see Student Training Program under Project Activities) will be tested using a larger scale-up population of ~400 students per semester at WFU. Also, instead of using BioTAP as an endpoint assessment tool for students’ theses, BioTAP tools will be used to evaluate lab reports in introductory level biology courses. The current proposal extends the original BioTAP training model in two other ways. First, before students write their first report, they will use the rubric with which their work will be evaluated to score published reports from primary literature, and score training reports written by the project team that highlight specific writing errors that are specifically focused on by the BioTAP rubric. This “reading-first” method has been used successfully for 3 years now at WFU. Average draft report grades have risen 4-5% after implementation of the reading exercise (PI’s pers. obs). It also is the method used in the original BioTAP protocol to train GTAs (Reynolds et al., 2009) and by STEM instructors elsewhere (e.g. Coil et al., 2010).

C. Project Activities
C.1. Overview
Our long range mission is improving scientific writing of post-secondary students in a range of programs and institutions. This goal requires breaking down implementation barriers that make it difficult to use scientific writing as a routine teaching and learning activity in STEM courses. Figure 3 below summarizes our preliminary theory of action for reaching this goal. 

The 4 intermediate goals (IGs) are the primary end points for this project and main points for assessment. For students to become confident and accomplished science writers they need to see clear, measurable progress (IG1) and believe in their ability to accomplish writing related tasks (IG2). What students need to reach these goals are clear, well-understood expectations (Requirement R1) and regular opportunities to practice, obtain positive growth- oriented feedback, and make corrections successfully (R2). Meeting these requirements assumes students will accept and act on the feedback they receive (Assumption A1), 
For GTAs to become effective scientific writing teachers they need to be able to facilitate an effective, evidenced based training process (IG3). This theory of action further posits that increasing GTAs' grading efficiency and consistency is an important positive motivator that encourages use of evidence-based scientific writing training in the future (IG4). These goals require GTAs have an evidence-based model they can use (R3), are trained how to use it (R4), and possess sufficient pedagogical knowledge to integrate these resources into the undergraduate learning process (R5). GTAs also need course infrastructure that allows and supports the methods they have been trained to use (R6-R7). Finally, GTAs must modify their approach to writing instruction by focusing efforts on higher order and global writing issues, not mechanical corrections or copy-editing (R8).
Given this overall theory of action, the central research question for this project is whether an integrated writing training program that combines text reading and annotation exercises, initial automated feedback on writing, and follow-up feedback from instructors that builds on the automated feedback, can meet these essential requirements, and thereby: 
•	accelerates student progression to mastery of scientific writing skills;
•	improves student confidence in their scientific writing skills;
•	improves instructors’ grading consistency while reducing time spent grading;
•	makes the assessment process more transparent; and
•	encourages adoption of evidence-based scientific writing practices.

The project will use a quasi-experimental design (Cook et al., 1979) to compare a historical control group with two interventions: 1) reading annotation exercises plus process-oriented coaching by GTAs; 2) preceding writing activities plus automated feedback provided by SAWHET. 

C.2. Timeline For Major Activities

 

C.3. Student Training Program (D. Johnson, S. Setaro)
The timeline in figure 4 summarizes when different activities take place during the funding period. The Reading Cycle (Fig. 5), developed by D. Johnson and S. Setaro and conducted by GTAs, will occur over a 2-week period once each semester, prior to students’ first writing assignments. Reading Cycles will be introduced in Project Year 1, and repeat each semester. Writing Cycles (fig. 5), conducted by GTAs, will occur twice each semester, once near mid-term, and again near end of the term, starting Project Year 1. Feedback from SAWHET will be added to existing Writing Cycles in Project Years 2 and 3. 
Reading Cycle: Student reading exercises (summarized as Reading Cycle in Fig. 5) will be implemented in the three biology 100- and 200-level courses (Ecology and Evolution, Genetics, Cell Biology). This cycle introduces undergraduates to the structure of scientific papers and important universal features of scientific writing, before they start writing. Undergraduates will use the BioTAP rubric to evaluate excerpts from student writing that illustrate unacceptable, acceptable, and excellent examples for each of the nine writing criteria in the BioTAP rubric. This activity is the only intervention conducted during the first two semesters (Year 1). The rest of the lab schedule follows that of previous years.
During the first weeks of each semester, students will read 1-2 primary scientific literature articles. Articles selected must be well written, relatively easy to read, and available under a Creative Commons license. Students will identify the most important components of scientific texts, which are already compiled in the BioTAP rubric, by annotating those parts in the text with CrowdFlower® (https://www.crowdflower.com/use- case/data-categorization/). CrowdFlower® is a free web-based crowdsourcing platform for managing text annotation. After students complete an introductory session about format and style of scientific writing, they annotate scientific papers based on these criteria from BioTAP for homework. GTAs will check and grade the students’ annotations and discuss the results during a subsequent lab meeting. For the next week, students will read two additional “training reports” that contain specific examples of technical flaws identifiable using the BioTAP rubric:
1.	Does the paper clearly articulate research goals? Use annotation tags to mark it in the paper.
2.	Is the paper clearly organized? Mark text passages that do not seem to be in the proper section.
3.	Does the paper contain a hypothesis? Annotate it in the paper.
4.	Does the paper interpret the results? Mark where authors are interpreting their results.
5.	Are data clear and informative? Mark points where this is not the case.
6.	Is supporting literature used to back up statements? Mark in the paper where support is missing. 
Students will annotate the training documents online using the same criteria that will be used to score their own written reports later in the semester. Training reports will be generated using prior student papers (with their permission) as a starting point. Both training articles and training reports will be uploaded to CrowdFlower®, so other instructors and institutions can use them as well. 
	The Reading Cycle activities train students to be specific and pay close attention to details, because they have to back up their statements with explicit examples from the text. The project team will be testing the hypothesis that this activity better equips students to recognize overall patterns of scientific writing, and to distinguish scientific writing from other genres. More importantly, we are testing whether students begin incorporating these concepts into their own writing earlier than students in the historical control group.
	Writing Cycle. Towards the middle of the semester, students will conduct their own experiments in lab then report their results in the form of a primary article. This triggers the first writing cycle (Fig. 5). Students submit a draft of their report first, before submitting a final version. Drafts are extensively commented on by GTAs and students have an opportunity to revise before submitting final versions. Shortly after students get back grades for their first lab report, their second experiment of the semester is due and a second writing cycle begins.
	Writing Cycle + SAWHET. In Years 2 and 3, students will complete the same reading and writing exercises as in Year 1 with the addition of feedback from SAWHET to improve their draft first before submitting the final version to the GTA (writing cycle + SAWHET, Fig. 2). All other components remain the same as previous years.
	End-of-Semester Evaluations. At the end of each semester, students complete their regular end of course evaluation of the lab course and their GTA. This evaluation contains a subset of questions from the Biology Self-Efficacy Scale (Baldwin, 1999) and the Colorado Learning Attitudes about Science Survey (Semsar et al., 2011). Students also will be asked on each end-of-course evaluation survey about how often they used online resources and automated feedback, when they used SAWHET prompts, how transparent the grading process was etc.

C.4. Graduate Teaching Assistants Training, Evaluation, Certification (D. Johnson, S. Setaro, J. Reynolds)
Training, classroom, and formal assessment activities for GTAs are depicted in Figure 6.
New GTA Training. Dr. Johnson has primary responsibility for GTA training and management for this project. He will conduct GTA training at the start of each academic year when new GTAs enter the departmental program. Dr. Reynolds’ GTA/rater training model developed at DUKE University will be used and divided into 3, 3-hour group sessions plus a ~1-hour inter-session activity. GTAs will also practice with all technology tools to ensure they understand how to them in the classroom. 
Both new and currently active GTAs will be required to attend training sessions in Year 1. In Years 2 and 3, only new GTAs entering the department will need to participate. Training Session 1 will outline the project and Certification activities, conduct informed consent, introduce the evidence base and writing instructional practices from Writing Across the Curriculum/ Disciplines, and outline how project activities are integrated into current lab coursework. In Training Session 2, GTAs will use the BioTAP rubric to evaluate excerpts from student writing that illustrate unacceptable, acceptable, and excellent examples for each of the nine writing criteria in the BioTAP rubric. Before Session 3 GTAs will use CrowdFlower® markup tools to evaluate additional writing samples. Training Session 3. In this final session, GTAs will be asked to collaborate to identify ways they might improve their scoring technique. Session 3 ends when GTAs complete GTA Knowledge Assessment 1, the first of three scheduled for the coming year. GTA Knowledge Assessments 1 and 2 are short quizzes to assess how well they learned best-practises of teaching writing. 
Classroom Observation 1. Two weeks before fall midterm, GTAs lead their lab sections through an instructional session on scientific writing. One of each GTA’s class presentations will be videotaped, and the session scored for adoption of project training practices using the Classroom Observation Protocol for Undergraduate STEM (Smith et al., 2013); additional notes and observations will be added by the PI/Co-PI. A summary of observations and recommendations for improvement from the PI and Co-PI will be returned to the GTA. Videos will be archived for future use in the study. 
Writing Cycles 1, 2; Interim Grading Checks. GTAs lead undergraduates through complete 2 writing cycles each semester, the first at mid-term and the second ~2 weeks before the end of term. Both cycles follow the same pattern. Students submit draft reports, which GTAs will score using the BioTAP rubric. GTAs add comments and suggestions for improvement to the electronic document and report them, including the time spent on grading, to the project team via web form. Commented drafts are returned to students. Approximately 1 week later, students submit final reports, which also are scored using the BioTAP rubric. Starting Year 2, students will begin receiving additional feedback comments on both draft and final reports from the automated SAWHET system.
End-of-Semester Evaluations. At the end of fall semester in Yrs 1-3, GTAs complete Knowledge Assessment 2, and the GTA Experience Survey for the first time. Questions are similar on Knowledge Assessments 1 and 2, focusing on how well GTAs learned BioTAP training program concepts and skills. The GTA Experience Survey is a set of Likert and open-response questions regarding their overall user experience with SAWHET and BioTAP. The Experience Survey is not intended as a data source for the study, but mainly to inform further development and improvement of the SAWHET interface and functions, and refinement of the training and administration protocols.
 Spring Semester Activities. Spring semester follows a similar pattern. GTAs will be taped a second time as they lead a writing training session. Three reports will be pulled randomly for each participating GTA during the first writing cycle; unlike fall semester, a second set of 3 reports will not be collected for Writing Cycle 2. At the end of spring semester, GTAs complete the GTA Experience Survey again, and Knowledge Assessment 3/ GTA Self-Efficacy Survey. This assessment is the final requirement for GTA certification. Assessment 3 will be of a case scenario or challenge that the GTAs are not specifically trained to handle. GTAs will receive a sample of poor student writing to evaluate, and a series of open-response prompts. GTAs' responses will be evaluated based on their accuracy in identifying writing problems, and how the feedback they provide to the “case student” utilizes principles from the BioTAP model. The self-efficacy portion consists of questions modified from the Biology Self-Efficacy Scale and the Colorado Learning Attitudes about Science survey that probe GTA mindset and likelihood to adopt best practices and tools from the project. 
Independent Grade Monitoring. WFU Biology independently monitors GTA grading performance each semester using linear regression correlations of final lecture and lab course grades (Gass, et al, 2013). Historically overall correlations between final lecture and lab course scores have ranged from 0.80-0.85, with <5% variation in slopes and intercepts. Final GTA lab grades falling >1 s.d. outside historical ranges are flagged and reviewed for potential calculation or reporting errors. The same criteria will be used to flag GTAs for interim grade check as described above in “Interim Grading Checks.”
Final Project Certification. GTAs who complete all project assessments, demonstrate inter-rater reliability >0.8 and provide a well-reasoned, empirically supportable solution to the final challenge problem will receive a BioTAP Project Certificate affirming that they meet the following skills criteria. They can:
•	Accurately present central instructional principles of a writing training model to students during group instruction. 
•	Implement training methods and rubrics accurately, and as originally designed. 
•	Provide undergraduates with substantive, actionable feedback and comments that reflect the central principles of an empirically supported writing model.
•	Grade with high inter-rater reliability.

Although this is not an official WFU program certificate, most WFU GTAs actively seek out documented skills as part of their overall professional development strategy. Also, while not a formal outcome for this project we do intend to look for ways to formalize BioTAP certification. One option we will explore is collaboration with WFU’s Teaching and Learning Center, which currently operates a general Teaching Certificate Program for GTAs. In addition to certification, we will make copies of GTAs’ teaching sessions available to them to include in job application materials or a teaching portfolio. GTAs also will be encouraged to carry our resources with them as they move into faculty positions at other institutions. 

C.5. Data Compilation and Cross Checking (Johnson and Setaro)
Drafts and final written reports from Writing Cycles 1 and 2 each semester will be collected through the SAWHET framework from all students in all three lab classes. To reiterate, SAWHET is not used for feedback in Year 1, only to collect and store lab reports. Lab reports will be stored in a dedicated, Mímir repository (Tablan et al. 2015), which allows semantic and linguistic queries and is scalable to gigabytes of text (Greenwood 2001). Feedback from GTAs on draft reports as well as time spent grading, will be collected with secure web-forms through Qualtrics. Graduate teaching assistants will enter the final grade for a specific lab report including all comments and whether undergraduates acted on these comments successfully. Survey data (Likert-scaled and open-response data collected from GTAs and students about their experience with the reading and writing exercises, self-efficacy in regards to writing activities, etc.) also will be exported and stored in Mímir databases. 
Users’ self-reported estimates of online resources use will be cross-checked against data from item-level web analytics and server access logs. Reporting accuracy of GTAs will be randomly spot-checked by D. Johnson using the same 3 randomly selected reports used for interim grades checking. Briefly, duplicates of the 3 commented draft and final lab reports for each GTA will be text-coded for qualitative and mixed methods data analysis using MaxQDA (VERBI Software GmbH, Berlin). GTA comments on reports will be tagged using a code tree constructed from the BioTAP rubric and training tools. Coded segments of GTA comments on each draft report will be overlaid onto matching final reports, and the final reports coded for presence or absence of changes recommended by GTAs. Uptake rates will be calculated for each report independently then compared to GTA rates using repeated measures ANOVA. GTAs reporting student uptake significantly (p<0.05) higher or lower than the PI will be notified of the variance and asked to re-evaluate reported scores. GTAs who are scoring significantly differently from the PI also will be flagged for follow-up in the subsequent semester. GTAs will be ineligible for certification until they can post uptake scores that are statistically equivalent to an independent monitor’s scores. While this is an important control check, in practice we do not expect this will be a problem; as a matter of pride most GTAs already check students’ final reports very closely to ensure their recommendations are incorporated. 

C.6. SAWHET Software Development (Setaro, Francom)
We will adapt SAWHET to match the BioTAP rubric starting Spring 2017 and throughout the summers of Years 1 and 2(Figure 7.) We will use the texts that have been annotated by students during the reading cycle and lab reports with GTAs’ comments that have been annotated by two undergraduate students working with us. Two-thirds of the corpus will be used as our training data set and one-third as our testing set. The testing data set will be restricted to testing only and will not be exposed to the trained model in order to avoid overfitting of the model algorithm. We will calculate a wide set of linguistic measures for the lab reports (lexical, grammatical, semantic, and stylistic). A predictive machine learning model will be created using a decision tree algorithm to identify and rank those linguistic measures that are most indicative of particular BioTAP rubric outcomes. Decision trees are particularly suited for the current task as they can handle continuous and categorical features as inputs, provide robust models with either few or many training examples, and return readily interpretable rule-based results than can be quickly implemented (Lantz, 2013). Also, the newly collected data will form the basis for refinement of our rule-based recognition system that is already implemented in SAWHET, to increase sensitivity (true positive rate) and specificity (true negative rate) of the SAWHET algorithms. For this, we will measure the performance of the SAWHET feedback in comparison to the GTA rubric feedback using a confusion matrix. Initially, we will adopt a rubric in SAWHET if the precision is at least 0.8; this means that in 80% of all actual problems, SAWHET catches the problem for that particular rubric. We are less concerned with low sensitivity of SAWHET because we argue SAWHET can be of value as a first-response feedback system even if not all problems are caught. However, we assume that low specificity would be a bigger problem as it could annoy students if the system flags incorrectly and assumes problems in their lab reports that are not present. 

We anticipate that the rule-based approach will be more appropriate for simpler rubrics that only look for certain components, for example, whether students discuss hypotheses, use appropriate statistics and cite primary literature in their writing. In fact, GATE rules have already been effective in detecting the presence of hypotheses and citations in lab reports of our historical cohort. The decision-tree based machine learning approach will be critical to more complex categories, but it has been shown that, given enough data, this approach can handle complex situations. Independent on the machine learning approach, though, having only a subset of feedback options would allow us to test the reactions of students in comparison with GTAs’ feedback and should help GTAs to spend less time on grading.
Both, revised rule-based algorithms and new machine learning results will be implemented in SAWHET for students to use starting Fall 2018 and will be refined in the following years. At the end of the project, we will package SAWHET, so that is available and easily adoptable to other institutions that use a similar instructional design in their biology labs.

C.7. Data Analyses (Johnson and Setaro)
We will begin analyzing data collected during the Fall semester in 2017 and Spring semester 2018 (Group A) and compare it to historical data collected in 2016 and Spring 2017 (Group 0). Specifically, we are looking for the difference in quality of writing based on criteria in the BioTAP rubric, in draft and final reports, before and after the instructional writing intervention. For this, we will grade 200 draft-final pairs from Group 0 by assigning 0 or 1 scores for each BioTAP rubric item. This is the same grading scheme GTAs will use throughout the project. We will then compare the difference between Group 0 and a random subset of 200 draft-final pairs from Group A using a Mann-Whitney U test. The Mann-Whitney U test assumes two categorical groups, independence of observations and the dependent variables at a continuous level. This test does not assume normally distributed data. We hypothesize that drafts of Group A will be of higher quality than those of Group 0. Results of this analysis will address our first goal “Accelerating student progression to mastery of scientific writing skills,” as outlined on page 1 of the project description. We do not expect significant difference between final reports of Group A and Group 0, because Group 0 had extensive opportunity to discuss their drafts with GTAs and get high-quality feedback. To assess if our approach “reduces the time instructors spend grading” (third goal, outlined on page 1), we will compare time spent on grading for GTAs in Group 0 and in Group A with Student’s t-tests. The 2 random spot samples we collected from every GTA of Group A, will be used to assess our goal to “improving instructors’ inter-rater reliability.” Therefore, we will calculate and compare the Krippendorff’s Alpha Coefficient as a measure of inter-rater reliability for each BioTAP rubric category of Group 0 and Group A. Krippendorff’s Alpha is used in content analysis to measure coding agreement in open-ended text data among different coders (Krippendorff, 2007). It can handle small sample sizes and data sets with missing data, which is suitable for our dataset, because not all lab reports will be graded by all GTAs.

In year two, we will compare data from Group A with data collected during Fall 2018 and Spring 2019 (Group B), to see if improvement from draft to final lab report decreased, increased or remained the same according to the BioTAP rubric. 
To assess whether there is a significant difference in grading time, we will compare grading times collected from GTAs of Group A and Group B. Inter-rater reliability differences between Group A and Group B will be assessed with Krippendorff’s Alpha Coefficient.
In the last year, we will analyze whether the improvements on SAWHET to the year before had a measurable effect or not. For this, we will use the same technique as in Spring 2019, just with data from Group B and Group C (data collected in Fall 2019 and Spring 2020), instead of Group A and Group B. This gives answers to the following question we raised before: “undergraduates response to automated feedback and does it help towards mastery”.
We will also look for correlations between when and how often students make use of supplemental assistance (i.e., automated feedback, instructor feedback, and online writing support resources), progression towards mastery, and self-efficacy in regards to writing-related tasks. This will address our second goal “Increasing student self-efficacy towards scientific writing activities”. Students’ attitudes and self-confidence about writing-related tasks will be measured using survey questions adapted from the Colorado Learning Attitudes about Science Survey For Biology (Adams, 2004), and Biology Self-Efficacy Scale (Baldwin, 1999). 
GTA attitudes will be measured using a scenarios-based survey instrument that we recently developed (Martin and Johnson, manu. in prep). Survey data will be analyzed using MaxQDA for qualitative data analysis. These results will help us monitor progress towards to our fifth goal of “Encouraging instructor adoption of evidence-based scientific writing instructional practices.” 

C.8. Suitability of Project Team and Study Site
Dr. Johnson (PI) is a Teaching Professor in Biology whose scholarly work focuses on developing tools and strategies for bringing evidence–based teaching into college STEM courses. As Core Curriculum Coordinator for Biology, he oversees end-of-course lab and GTA evaluations as well as the department’s general curriculum assessment exam. In this position, he has direct control over all of the historical data needed for the control arm of the study. As Biology GTA supervisor and past director of WFU’s graduate professional development program, he has nearly 20 years’ experience developing and delivering GTA training programs. 
Dr. Setaro (Co-PI) has over 5 years of educational program management experience. Currently, she is the Program Coordinator of BioBook, an electronic teaching and learning tool for biology students developed at WFU. Before this, she was the evaluation coordinator UNTRAC (Undergraduate Neuroscience Training Cooperative), a joint educational program sponsored by WFU and Winston-Salem State University. Over the last 6 years she has collected, curated and analyzed student data from many different sources (texts, system user data, surveys, classroom observations) so is very familiar with the challenges it presents and knows how to overcome them. She also brings an exceptional range of programming and data analysis skills to the team. She has been the lead developer and programmer for our SAWHET platform, and is proficient with Java, Python, GATE and R. Dr. Setaro's earlier work on fungal ecology and phylogenetics is the source of her proficiency with a broad range of analytical and statistical approaches (clustering algorithms, Maximum Likelihood and Bayesian approaches, network analyses), which will be critical to the assessment arm of this project.
Dr. Francom is an Associate Professor of Spanish & Linguistics at WFU. His expansive linguistics and lexicographic analysis experience brings new insights into the questions raised by this project. His research focuses on using large-scale language archives from a variety of sources (news, social media, and other internet sources) to better understand the linguistic and cultural similarities and differences between varieties of the Spanish language (native dialect and non-native/ learner language) for both scholarly and pedagogical projects. He has published on topics including the development, annotation, and evaluation of corpora and explored computational methods such as text classification and clustering algorithms.
WFU has a long record of using innovative technology in the classroom; it was among the early adopters of a “laptops for all” program, and still requires all students have devices that meet minimum functional specifications. Thus we can assure that students will be able to access our digital tools. Moreover technology is ubiquitous in WFU courses, giving our undergraduates and graduate student a strongly “pro-technology” disposition, so they quickly adopt new tools with minimal resistance. This will help mitigate instructor and student resistance as potential confounding variable for this project. 
Biology’s Core Curriculum is a 4-course lecture/lab sequence enrolling ~500 students/semester. Regular faculty teach lecture sections with GTAs serving as lab instructors for 2+ semesters. Three lab courses have the same structure; undergraduates complete inquiry activities lasting 2-3 weeks ending with their own experiments. Results for 2 experiment/semester are reported in formal lab reports formatted as standard scientific papers. 

While not a formal part of this proposal, we also have an opportunity to begin evaluating how the training model can be scaled up. Beginning January 2017, WFU Biology will be expanding to a second branch campus. Within 5 years, enrollment is projected to double in 2 of the 3 courses involved in this project. This expansion will provide insights into scale-up challenges that would need to be addressed in any subsequent Tier II Scale-Up project proposals. 

C.9. Project Oversight and Student Privacy
Project activities involving students and GTAs are subject to review and oversight of WFU’s Institutional Review Board and will be conducted in accordance with federal regulations regarding human subjects research. In-class activities involving undergraduates will be independently reviewed and approved by the WFU Dept. of Biology Undergraduate Studies Committee. Similarly, GTA training activities will be independently reviewed and approved by the WFU Department of Biology Graduate Program Committee. Letters of support from the current chairs of both committees have been appended to the proposal.

D. Intellectual Merits
New ed-tech tools are appearing at an ever-faster pace, but there is limited well-controlled research showing they have a positive effect on learning outcomes. This project will contribute much-needed information on how technology supports development of higher level thinking skills, in this case, writing skills development. The project is a controlled comparison of student learning outcomes with versus without technology support in the context of a well-defined instructional intervention. Previous studies have relied heavily on self-reports of perceived benefits of technology assistance. At the end of this project, we will be able to describe in detail how much automated feedback students actually use. GTAs will be reviewing student work and reporting specifically what feedback their students act on; GTA reports in turn will be cross-checked using qualitative text analyses. Although not a primary outcome goal of the project, we expect to begin identifying points of student and GTA resistance to use of technology support. These insights will be important for planning future dissemination and scale-up projects.
The project will produce tangible assets that can support future basic research and training too. By the end of the 3-year NSF funding period we expect to have a fully operational SAWHET automated assessment and feedback technology platform programmed to evaluate student reports according to the BioTAP rubric. We hope will grow into a widely used tool that sparks further research on student writing in STEM by making the data collection process simpler. SAWHET uses open-source software and frameworks, so may be shared freely with the research and education communities. Its core was designed to be flexible and expandable. The algorithms we are creating to evaluate student writing can be implemented independently of SAWHET or GATE. All algorithms will be shared via GitHub or other portal as part of publications so they can be repurposed for other research questions.
This project will generate a structured corpus of ~ 4800 anonymized, annotated short lab reports written by undergraduate biology students. To our knowledge no similar structured corpus of student writing for STEM disciplines exists. By making the corpus available to the research community, this project will enable new studies that to date have not been possible. The corpus can be used as training data for linguistic analyses, to test other automated feedback systems, or for other research.

E. Broader Impacts
The Sourcebook for Advancing and Funding Undergraduate STEM Education (Fry, 2014) identifies seven overarching goals as key steps towards STEM reform; the current proposal directly speaks to four of them:
1. 	Support department-wide implementation of evidence-based practices.
2. 	Strengthen teacher preparation programs.
3. 	Support continued examination and adoption of successful curricular approaches.
4. 	Expect institutions to address systemic change through measurable indicators and evaluation.

 This study can potentially impact scientific writing skills of undergraduate students in a wide range of colleges and universities, thus improving post-secondary education. First, the proposed project addresses the broader need for evidence-based scientific writing education that brings proven writing practices into the biology classroom. Second, it connects adoptees to an enormous body of supporting resources from the Write Across the Curriculum, Writing in the Disciplines, and Write to Learn (WAC/WID/WTL) movements that began in the late 1970s and early 1980s. These programs have been extremely successful in promoting evidence-based writing instructional practices, and in making writing an integral part of post-secondary education. As a result, more than half of colleges and universities offer writing instruction and support based on these models (Thaiss, et al. 2010). Yet uptake has been uneven, with STEM being especially resistant. The writing training model proposed and tested here is based on WAC/WID/WTL principles of practice, and can help adoptees connect their in-class efforts to the larger WAC/WID/WTL community within their own institutions, and to external resources such as the WAC Clearinghouse (http://wac.colostate.edu/). 
Results of this study also have the potential to improve GTA training and development. There is clear need for research on GTA training strategies. Currently, the majority of GTAs have a one-time workshop as their main form of training. Many institutions observe and provide feedback to their graduate students about their teaching, but most would like more support for programs and assessment of programs (Schussler, et al. 2015; https://biotap.utk.edu/.) Recently Dr. Schussler received an NSF award (RCN-UBE Award #1539903; 2015-2020) to build a sustainable collaborative network to support, synthesize, and disseminate research on biology GTA teaching professional development, and to advocate for evidence-based training standards. The current project will address these issues directly. Moreover, The PI for the current proposal (Johnson) is a member of Dr. Schussler's Advisory Board, and plans to share findings from our project with the larger community as quickly as possible through this RCN pathway.
It should be noted here that the methods being tested are not restricted to GTAs alone. With minimal changes, this model could be used in AP Biology classes in high school, or in higher level college classes. The unifying nature of scientific writing should also make it easy to use the writing training model across all STEM fields. We are confident that most teachers can learn to use our 3-element training model and to interpret data provided by the SAWHET system.
While not a main point of this project, we will be interested to see whether this training approach reduces “STEM flight,” that is, loss of large numbers of students from STEM majors during and after their first year (President’s Council of Advisors on Science and Technology, 2012.) 
 
