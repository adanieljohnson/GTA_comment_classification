---
title: "Analyzing TA Comments on Student Writing in R"
---

```{r, child="_setup.Rmd"}
```

## Background
I am interested in understanding how students develop technical writing skills in biology, and more specifically, how to apply data science methods to improve that process. 

My current NSF-sponsored project asks if and how scripted instruction, automated support, and holistic feedback accelerates student development as writers. One specific aim is to see if changing the structure and focus of instructors' comments affects how students' writing develops over time. To measure this we extract instructor comments from student reports, classify those comments, and correlate the types of comments TAs make with student performance. 

My project for the Faculty Learning Community for R is partly exploratory, in I am asking: what information is present and can be mined from the TA comments dataset? The project also is partly predictive in that I hope to build a text classification workflow that automates the process of categorizing TA comments on student writing.

#### \   
## Prior Work
In Summer 2018, we extracted and tabulated ~11,000 TA comments from ~500 pairs (initial submissions and revised version) of short lab reports written by undergraduates in 3 different general biology lab courses collected during a single semester. Using an iteratively generated codebook, all 11,000 comments were classified in two categories:

* __Subject__: did the comment focus mainly on Basic criteria, Technical issues, Writing quality, Logic and thinking, or Other issues
* __Structure__: did the comment Point to an error without providing other help, provide simple Copy Correction, provide declarative General or Specific Information only, or provide Holistic coaching that fosters student thinking and long-term improvement?

#### \   
Frequency counts of unambiguously coded TA comments were sorted into a two-axis contingency table that provided two important insights: 

* A snapshot of the most common types of comments TAs made, and 
* A relative estimate of where TAs are placing their greatest effort.

(Embed a copy of that table here.)

#### \   

## Specific Project Goals
Unix scripts can extract and compile TA comments and meta-data for each student report into a CSV datafile. However routinely classifying TA comments by hand is impractical; 10-12,000 comments need to be scored EACH SEMESTER. An automated classification method is needed both for speed and consistency.

My specific project goals were/are to:

1. **Create an R-based workflow that converts TA comments and metadata from student reports into a de-identified "tidy" dataframe for R.** This includes converting the master CSV data set to an anonymized data frame with all names and confidential metadata re-keyed to unique random identifiers.
2. **Use text features to assign TA comments to the hand-coded categories defined in our existing codebook**. The goal is to be able to assign instructor comments into our pre-defined Subject and Structure codes with 90% accuracy or greater. 
3. **Assemble these methods into a semi-automated text classifier**. The final classifier model will be able to generate a two-axis contingency table similar to the one created using hand-coded data. It will be applied to newer datasets to determine if and how TA comments change over semesters. 

#### \   
