---
title: "Automated Classification of TA Comments on Student Writing"
---

```{r, child="_setup.Rmd"}
```

## Overview
I am using classroom research and data science to better understand how students develop technical writing skills in large-enrollment biology courses.

My current NSF-sponsored grant asks whether we can accelerate student development as writers using a mix of scripted active instruction, automated support of writing in progress, and holistic feedback. One specific question we have is how **changing the structure and main focus of instructors' comments** affects students' writing development over time. To answer this we extract instructor comments from student reports, categorize the comments according to their subject and structure, then correlate the types of comments instructors make with student performance. 

#### \   
## Prior Work
In Summer 2018, we extracted and tabulated ~11,000 comments made by teaching assistants (TA) from >500 pairs (initial submissions and revised version) of short lab reports written by undergraduates in 3 different general biology lab courses during a single semester. Using an iteratively generated codebook, each of the comments was separately classified in two categories:

* __Subject__: did the comment focus mainly on **Basic Criteria, Technical Issues, Writing Quality, Logic and Thinking,** or **Other** issues
* __Structure__: did the comment **Point** to an error without providing other help, provide simple **Copy Correction**, provide declarative **General or Specific Information** only, or provide **Holistic Coaching** that fosters student thinking and long-term improvement?

#### \   
Counts of coded TA comments were tabulated according to subject and structure, and frequencies for each subject/structure pair calculated as a fraction of all comments. This 2-way contingency table provided us with a direct estimate of the most common types of comments TAs made on student work, and a relative estimate of which elements of writing TAs emphasize most often. 

Classifying TA comments this way is informative, but doing so by hand is impractical: 10-12,000 comments need to be scored EACH semester. There also is a significant risk of "coding drift" over time, reducing accuracy.

#### \   

##Specific Project Goals
My project for the **Faculty Learning Community for R** was to create an automated comment classifier that could generate two-axis contingency tables comparable to the one generated using hand-coded data. This automated classifier will used within the larger NSF-funded research project to:

*  Improve both speed and consistency when classifying TA comments across multiple semesters. 
*  Monitor how the types and focus of TAs comments on student writing change over time. 

#### \   
## Site Map
Insert a text-based overview and site map of the major pages and branch points here.


#### \   
