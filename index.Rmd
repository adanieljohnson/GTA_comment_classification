---
title: "Automated Classification of TA Comments on Student Writing"
---

```{r, child="_setup.Rmd"}
```

##Overview
I am using classroom research and data science to better understand how students develop technical writing skills in large-enrollment biology courses.

My current NSF-sponsored grant asks whether we can accelerate student development as writers using a mix of scripted active instruction, automated support of writing in progress, and holistic feedback. One specific question we have is how **changing the structure and main focus of instructors' comments** affects students' writing development over time. To answer this we extract instructor comments from student reports, categorize the comments according to their subject and structure, then correlate the types of comments instructors make with student performance. 

Classifying TA comments this way is informative, but doing so by hand is impractical: 10-12,000 comments need to be scored EACH semester. There also is a significant risk of "coding drift" over time, reducing accuracy.

My project for the **Faculty Learning Community for R** was to create an automated comment classifier that could generate two-axis contingency tables comparable to the one generated using hand-coded data. This automated classifier will used within the larger NSF-funded research project to:

*  Improve both speed and consistency when classifying TA comments across multiple semesters. 
*  Monitor how the types and focus of TAs comments on student writing change over time. 

#### \   
##Work Plan
The work outline for this project is:

1.  Compile an initial dataset of hand-coded TA comments on reports for testing. Data will be de-identified by replacing both TAs' and students' names with anonymous IDs.
2.  Import TA comments and relevant metadata from student reports into R as a "tidy" dataframe.
3.  Use basic text analysis methods to explore comment data structure, n-gram frequencies, etc., and identify potentially useful elements for feature engineering.
4.  Write a workflow using Naive Bayes for supervised text classification, and test various permutations of analysis parameters to establish a baseline classifier.
5.  Assign TA comments to categories using the baseline NB classifier, and establish baseline accuracy values by comparing outcomes of automated classification to hand-coded results.
6.  Using features (from Step #3) and comparison data from NB classification protocol (from Step #5) as guides, further optimize the NB classifier method until predicted accuracy stabilizes.
7.  Apply the optimized NB classifier to the original FULL comment dataset, identify all comments that are being classified incorrectly, and look for potential patterns.
8.  Write a small set of rule- or REGEX pattern-based searches that can identify comments which are more likely to be classified incorrectly. 
9.  Combine the rules/pattern-based pre-screening process with the optimized Naive Bayes classifier to create a vertical Ensemble Classifier.
10.  Re-validate the Ensemble Classifier using a subset of ~2000 new hand-coded TA comments extracted from student lab reports for a different semester than the initial test dataset. Compare automated results for new data set to hand-coded categories.

Looking beyond the current project, I will use the vertical Ensemble Classifier two ways:

*  To monitor TA comment patterns in the ongoing NSF-funded project.
*  As the control method for evaluating other potential text classifier strategies. 


#### \   
##Project Site Map
Insert a text-based overview and site map of the major pages and branch points here.

When appropriate, anchor pages branch to extended supporting documentation.

#### \   
***
