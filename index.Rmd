---
title: "Analyzing TA Comments on Student Writing in R"
---

```{r, child="_setup.Rmd"}
```

## Background
I am interested in understanding how students develop technical writing skills in biology, and more specifically, how we can apply data science methods to improve that process. 

My current NSF-sponsored project asks if and how scripted instruction, automated support, and holistic feedback accelerates student development as writers. One of our specific aims is to see if changing the structure and focus of instructors' comments affects how students' writing develops over time. To do this, we need to extract instructor comments from student reports, classify those comments, and correlate the types of comments TAs make with student performance. 

My project for the Faculty Learning Community for R is partly exploratory, in I am asking: what information is present and can be mined from the TA comments dataset? The project also is partly predictive in that I hope to build a text classification workflow that automates the process of categorizing TA comments on student writing.

#### \   
## Prior Work
In Summer 2018, we extracted and tabulated ~11,000 TA comments from ~500 pairs (initial submissions and revised version) of short lab reports written by undergraduates in 3 different general biology lab courses collected during a single semester. Using an iteratively generated codebook, all 11,000 comments were classified in two categories:

* __Subject__: did the comment focus mainly on Basic criteria, Technical issues, Writing quality, Logic, or Other issues
* __Structure__: did the comment Point Out an error without providing other help, provide simple Copy Correction, provide declarative General or Specific Information only, or provide Holistic coaching that fosters student thinking and long-term improvement?

#### \   
Frequency counts of unambiguously coded TA comments were sorted into a two-axis contingency table that provided two important insights: 

* A snapshot of the most common types of comments TAs made, and 
* A relative estimate of where TAs are placing their greatest effort.

#### \   
## Specific Project Goals
TA comments and meta-data for each student report can be extracted and compiled into a CSV datafile using existing Unix scripts. However routinely classifying TA comments by hand is impractical; 10-12,000 comments need to be scored EACH SEMESTER. An automated classification method is needed both for speed and consistency.

My specific project goals were/are to create an R-based text analysis workflow that:

1. **Converts TA comments and metadata into a de-identified "tidy" dataframe for R.** This includes converting the master CSV data set to an anonymized data frame with all names and confidential metadata re-keyed to unique random identifiers.
2. **Uses text features to assign TA comments to the categories defined in our existing codebook**. The goal is to be able to assign instructor comments into our pre-defined Subject and Structure codes with 90% accuracy or greater. 

#### \   
